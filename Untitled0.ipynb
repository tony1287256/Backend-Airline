{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrX5xWjv+Wa6SdGnR1pn8D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tony1287256/Backend-Airline/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-RN2AK1zkzy",
        "outputId": "cf61955b-6375-4418-e7a8-5447866a01e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m235.5/244.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rttI3tCjzPp4",
        "outputId": "7ac20e7c-82de-4ed0-a633-be7b947dd49c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversation 1:\n",
            "Sandeep:  One thing which I've taken away for the wealthy is that they are very serious about growth. They make sure that the money is compounding over a long period of time. The US by the way, that people are running out of money in their older age.\n",
            "Ranveer:  People are running out of money because of bad planning or because of inflation.\n",
            "\n",
            "Conversation 2:\n",
            "Sandeep:  No, they're in fact, they will live so long. Oh shit.\n",
            "Ranveer:  What do you think Adani and Mukesham money do with their own money? And by that I mean not reliance his money, Adani groups money, their own money as men.\n",
            "\n",
            "Conversation 3:\n",
            "Ranveer:  Now there's countless content pieces on YouTube on Spotify where you can learn about money, but it's these kind of conversations with experts from the world of finance people who manage money in the range of 50,000 rupees and savings to 50 crores and savings per month. That's the experience, the financial know how of our guest today. It's Sandeep Jaitwani on the show. His startup Dizerve.in is a modern day portfolio management service. Personally for me, most of my money is handled not by myself, but by portfolio management services. I trust this man enough to give him my own money. He is handling some of beer by himself media world's funds. That's why I thought that on this episode, let me give you guys access to someone who is given me financial advice, someone that I trust my money with. It's going to be a very raw and open conversation about everything from how the richest in India manages their money to how you can manage your money even if you're just beginning your career right now. It's a financial edu key.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import PyPDF2\n",
        "import docx\n",
        "import os\n",
        "\n",
        "def extract_conversation_from_file(file_path):\n",
        "    # Get the file extension\n",
        "    file_extension = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    if file_extension == '.csv':\n",
        "        return extract_from_csv(file_path)\n",
        "    elif file_extension == '.pdf':\n",
        "        return extract_from_pdf(file_path)\n",
        "    elif file_extension == '.docx':\n",
        "        return extract_from_docx(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please provide CSV, PDF, or DOCX file.\")\n",
        "\n",
        "def extract_from_csv(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df\n",
        "\n",
        "def extract_from_pdf(file_path):\n",
        "    pdf_text = \"\"\n",
        "    with open(file_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            pdf_text += page.extract_text()\n",
        "\n",
        "    return pdf_text\n",
        "\n",
        "def extract_from_docx(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    doc_text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "    return doc_text\n",
        "\n",
        "def process_conversation(file_path):\n",
        "    try:\n",
        "        extracted_data = extract_conversation_from_file(file_path)\n",
        "\n",
        "        # If the data is a DataFrame (CSV case)\n",
        "        if isinstance(extracted_data, pd.DataFrame):\n",
        "            # Dynamically extract all unique speakers\n",
        "            speakers = extracted_data['Speaker'].unique()\n",
        "            conversations = {}\n",
        "\n",
        "            # Group utterances by speaker\n",
        "            for speaker in speakers:\n",
        "                conversations[speaker] = extracted_data[extracted_data['Speaker'] == speaker]['Utterance'].tolist()\n",
        "\n",
        "            # Print conversations between speakers in a round-robin style\n",
        "            max_len = max([len(utterances) for utterances in conversations.values()])\n",
        "            for i in range(max_len):\n",
        "                print(f\"Conversation {i+1}:\")\n",
        "                for speaker in speakers:\n",
        "                    if i < len(conversations[speaker]):\n",
        "                        print(f\"{speaker}: {conversations[speaker][i]}\")\n",
        "                print(\"\")\n",
        "\n",
        "        # For text-based formats (PDF, DOCX)\n",
        "        else:\n",
        "            print(\"Extracted Text:\\n\", extracted_data)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing the file: {e}\")\n",
        "\n",
        "# Example usage\n",
        "file_path = '/content/Trancription.csv'  # Path to the uploaded file\n",
        "process_conversation(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RAxWdZsH-ca",
        "outputId": "4afb0a43-0dc5-4245-dd2a-e26c14739518"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m235.5/244.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import PyPDF2\n",
        "import docx\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import scipy.special\n",
        "import torch\n",
        "\n",
        "# Load FinBERT model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
        "\n",
        "def extract_conversation_from_file(file_path):\n",
        "    file_extension = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    if file_extension == '.csv':\n",
        "        return extract_from_csv(file_path)\n",
        "    elif file_extension == '.pdf':\n",
        "        return extract_from_pdf(file_path)\n",
        "    elif file_extension == '.docx':\n",
        "        return extract_from_docx(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please provide CSV, PDF, or DOCX file.\")\n",
        "\n",
        "def extract_from_csv(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df\n",
        "\n",
        "def extract_from_pdf(file_path):\n",
        "    pdf_text = \"\"\n",
        "    with open(file_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            pdf_text += page.extract_text()\n",
        "    return pdf_text\n",
        "\n",
        "def extract_from_docx(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    doc_text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "    return doc_text\n",
        "\n",
        "def finbert_sentiment(text):\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "            scores = scipy.special.softmax(logits.numpy().squeeze())\n",
        "            pos_score = scores[2]  # Assuming 'positive' is at index 2\n",
        "            neg_score = scores[0]  # Assuming 'negative' is at index 0\n",
        "            neu_score = scores[1]  # Assuming 'neutral' is at index 1\n",
        "\n",
        "            sentiment_label = \"neutral\"\n",
        "            if pos_score > neg_score:\n",
        "                sentiment_label = \"positive\"\n",
        "            elif neg_score > pos_score:\n",
        "                sentiment_label = \"negative\"\n",
        "\n",
        "            return {\n",
        "                \"positive\": pos_score,\n",
        "                \"negative\": neg_score,\n",
        "                \"neutral\": neu_score,\n",
        "                \"sentiment\": sentiment_label\n",
        "            }\n",
        "    except Exception as e:\n",
        "        print(f\"Error analyzing sentiment for text: {text}\\n{e}\")\n",
        "        return {\"positive\": 0.0, \"negative\": 0.0, \"neutral\": 0.0, \"sentiment\": \"neutral\"}\n",
        "\n",
        "def process_conversation(file_path):\n",
        "    try:\n",
        "        extracted_data = extract_conversation_from_file(file_path)\n",
        "\n",
        "        if isinstance(extracted_data, pd.DataFrame):\n",
        "            # Dynamically extract all unique speakers\n",
        "            speakers = extracted_data['Speaker'].unique()\n",
        "            conversations = {}\n",
        "\n",
        "            # Group utterances by speaker\n",
        "            for speaker in speakers:\n",
        "                conversations[speaker] = extracted_data[extracted_data['Speaker'] == speaker]['Utterance'].tolist()\n",
        "\n",
        "            # Print conversations with sentiment analysis\n",
        "            for speaker in speakers:\n",
        "                print(f\"\\nSpeaker: {speaker}\")\n",
        "                for utterance in conversations[speaker]:\n",
        "                    sentiment_result = finbert_sentiment(utterance)\n",
        "                    print(f\"Utterance: {utterance}\")\n",
        "                    print(f\"Sentiment: {sentiment_result['sentiment']} (Pos: {sentiment_result['positive']:.2f}, Neg: {sentiment_result['negative']:.2f}, Neu: {sentiment_result['neutral']:.2f})\")\n",
        "\n",
        "        else:\n",
        "            print(\"Extracted Text:\\n\", extracted_data)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing the file: {e}\")\n",
        "\n",
        "# Example usage\n",
        "file_path = '/content/Trancription.csv'  # Path to the uploaded file\n",
        "process_conversation(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yV_gxPCsHsXW",
        "outputId": "1fe106d3-eefc-43a1-cd41-a9e58896271a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Speaker: Sandeep\n",
            "Utterance:  One thing which I've taken away for the wealthy is that they are very serious about growth. They make sure that the money is compounding over a long period of time. The US by the way, that people are running out of money in their older age.\n",
            "Sentiment: positive (Pos: 0.43, Neg: 0.06, Neu: 0.52)\n",
            "Utterance:  No, they're in fact, they will live so long. Oh shit.\n",
            "Sentiment: positive (Pos: 0.90, Neg: 0.07, Neu: 0.03)\n",
            "\n",
            "Speaker: Ranveer\n",
            "Utterance:  People are running out of money because of bad planning or because of inflation.\n",
            "Sentiment: positive (Pos: 0.06, Neg: 0.01, Neu: 0.93)\n",
            "Utterance:  What do you think Adani and Mukesham money do with their own money? And by that I mean not reliance his money, Adani groups money, their own money as men.\n",
            "Sentiment: positive (Pos: 0.94, Neg: 0.03, Neu: 0.04)\n",
            "Utterance:  Now there's countless content pieces on YouTube on Spotify where you can learn about money, but it's these kind of conversations with experts from the world of finance people who manage money in the range of 50,000 rupees and savings to 50 crores and savings per month. That's the experience, the financial know how of our guest today. It's Sandeep Jaitwani on the show. His startup Dizerve.in is a modern day portfolio management service. Personally for me, most of my money is handled not by myself, but by portfolio management services. I trust this man enough to give him my own money. He is handling some of beer by himself media world's funds. That's why I thought that on this episode, let me give you guys access to someone who is given me financial advice, someone that I trust my money with. It's going to be a very raw and open conversation about everything from how the richest in India manages their money to how you can manage your money even if you're just beginning your career right now. It's a financial edu key.\n",
            "Sentiment: positive (Pos: 0.93, Neg: 0.05, Neu: 0.02)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_conversation(file_path):\n",
        "    try:\n",
        "        extracted_data = extract_conversation_from_file(file_path)\n",
        "\n",
        "        if isinstance(extracted_data, pd.DataFrame):\n",
        "            print(\"Initial DataFrame:\")\n",
        "            print(extracted_data.head())\n",
        "\n",
        "            # Sentiment analysis\n",
        "            sentiment_results = []\n",
        "            for index, row in extracted_data.iterrows():\n",
        "                sentiment = finbert_sentiment(row['Utterance'])  # Use 'Utterance' instead of 'Text'\n",
        "                sentiment_results.append((\n",
        "                    sentiment['positive'],\n",
        "                    sentiment['negative'],\n",
        "                    sentiment['neutral'],\n",
        "                    sentiment['sentiment']\n",
        "                ))\n",
        "\n",
        "            # Creating a DataFrame for sentiment analysis results\n",
        "            sentiment_df = pd.DataFrame(sentiment_results, columns=['finbert_pos', 'finbert_neg', 'finbert_neu', 'finbert_sentiment'])\n",
        "            extracted_data = pd.concat([extracted_data, sentiment_df], axis=1)\n",
        "\n",
        "            print(\"\\nDataFrame after Sentiment Analysis:\")\n",
        "            print(extracted_data)\n",
        "\n",
        "            # Detailed analysis\n",
        "            print(\"\\nDetailed Analysis:\")\n",
        "            for index, row in extracted_data.iterrows():\n",
        "                print(f\"Speaker: {row['Speaker']}, Utterance: {row['Utterance']}, Sentiment: {row['finbert_sentiment']}, Scores: (Pos: {row['finbert_pos']:.3f}, Neg: {row['finbert_neg']:.3f}, Neu: {row['finbert_neu']:.3f})\")\n",
        "\n",
        "            # Summary by speaker\n",
        "            summary = extracted_data.groupby('Speaker').agg({\n",
        "                'finbert_pos': 'sum',\n",
        "                'finbert_neg': 'sum',\n",
        "                'finbert_neu': 'sum',\n",
        "                'finbert_sentiment': lambda x: \"positive\" if (x == \"positive\").sum() > (x == \"negative\").sum() else \"negative\" if (x == \"negative\").sum() > 0 else \"neutral\"\n",
        "            })\n",
        "            summary['sentiment_score'] = summary['finbert_pos'] - summary['finbert_neg']\n",
        "\n",
        "            print(\"\\nSummary by Speaker:\")\n",
        "            print(summary)\n",
        "\n",
        "            # Overall sentiment report\n",
        "            overall_score = summary['sentiment_score'].sum()\n",
        "            overall_sentiment = \"positive\" if overall_score > 0 else \"negative\" if overall_score < 0 else \"neutral\"\n",
        "\n",
        "            print(\"\\nOverall Sentiment Report:\")\n",
        "            print(f\"Overall Score: {overall_score}\")\n",
        "            print(f\"Overall Sentiment: {overall_sentiment}\")\n",
        "\n",
        "        else:\n",
        "            print(\"Extracted Text:\\n\", extracted_data)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing the file: {e}\")\n",
        "\n",
        "# Example usage\n",
        "file_path = '/content/Summary.pdf' # Path to the uploaded file\n",
        "process_conversation(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fl8LqnFK9cJ",
        "outputId": "9107e348-84b4-4a7f-f455-351b9093f71c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing the file: name 'extract_conversation_from_file' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_cT3i3i_PdH",
        "outputId": "53e885a0-4abc-4bf0-c6d5-ce506e182389"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import PyPDF2\n",
        "import docx\n",
        "import os\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load FinBERT model and tokenizer\n",
        "model_name = \"yiyanghkust/finbert-tone\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "def finbert_sentiment(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    probabilities = torch.nn.functional.softmax(logits, dim=-1).numpy()[0]\n",
        "    sentiment = ['negative', 'neutral', 'positive'][probabilities.argmax()]\n",
        "\n",
        "    return {\n",
        "        'positive': probabilities[2],\n",
        "        'negative': probabilities[0],\n",
        "        'neutral': probabilities[1],\n",
        "        'sentiment': sentiment\n",
        "    }\n",
        "\n",
        "def extract_conversation_from_file(file_path):\n",
        "    file_extension = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    if file_extension == '.csv':\n",
        "        return extract_from_csv(file_path)\n",
        "    elif file_extension == '.pdf':\n",
        "        return extract_from_pdf(file_path)\n",
        "    elif file_extension == '.docx':\n",
        "        return extract_from_docx(file_path)\n",
        "    elif file_extension == '.xlsx':\n",
        "        return extract_from_excel(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please provide CSV, PDF, DOCX, or XLSX file.\")\n",
        "\n",
        "def extract_from_csv(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    if 'Speaker' in df.columns and 'Utterance' in df.columns:\n",
        "        return df[['Speaker', 'Utterance']]\n",
        "    else:\n",
        "        raise ValueError(\"CSV must contain 'Speaker' and 'Utterance' columns.\")\n",
        "\n",
        "def extract_from_pdf(file_path):\n",
        "    pdf_text = \"\"\n",
        "    with open(file_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            pdf_text += page.extract_text() + \"\\n\"\n",
        "    return pdf_text.strip()\n",
        "\n",
        "def extract_from_docx(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    doc_text = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
        "    return doc_text.strip()\n",
        "\n",
        "def extract_from_excel(file_path):\n",
        "    df = pd.read_excel(file_path)\n",
        "    if 'Speaker' in df.columns and 'Utterance' in df.columns:\n",
        "        return df[['Speaker', 'Utterance']]\n",
        "    else:\n",
        "        raise ValueError(\"Excel file must contain 'Speaker' and 'Utterance' columns.\")\n",
        "\n",
        "def process_pdf_conversation(pdf_text):\n",
        "    lines = [line.strip() for line in pdf_text.split('\\n') if line.strip()]\n",
        "    speakers = ['Interviewer', 'Financial Expert']\n",
        "    dialogue = []\n",
        "\n",
        "    current_speaker = None\n",
        "    for line in lines:\n",
        "        for speaker in speakers:\n",
        "            if line.startswith(speaker + \":\"):\n",
        "                current_speaker = speaker\n",
        "                utterance = line[len(speaker) + 1:].strip()\n",
        "                dialogue.append((current_speaker, utterance))\n",
        "                break\n",
        "\n",
        "    df = pd.DataFrame(dialogue, columns=['Speaker', 'Utterance'])\n",
        "    return df\n",
        "\n",
        "def process_conversation(file_path):\n",
        "    try:\n",
        "        extracted_data = extract_conversation_from_file(file_path)\n",
        "\n",
        "        if isinstance(extracted_data, pd.DataFrame):\n",
        "            print(\"Initial DataFrame:\")\n",
        "            print(extracted_data.head())\n",
        "            perform_sentiment_analysis(extracted_data)\n",
        "        else:\n",
        "            df = process_pdf_conversation(extracted_data)\n",
        "            print(\"\\nExtracted Dialogue DataFrame:\")\n",
        "            print(df)\n",
        "\n",
        "            perform_sentiment_analysis(df)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing the file: {e}\")\n",
        "\n",
        "def perform_sentiment_analysis(df):\n",
        "    sentiment_results = []\n",
        "    for index, row in df.iterrows():\n",
        "        sentiment = finbert_sentiment(row['Utterance'])\n",
        "        sentiment_results.append((\n",
        "            sentiment['positive'],\n",
        "            sentiment['negative'],\n",
        "            sentiment['neutral'],\n",
        "            sentiment['sentiment']\n",
        "        ))\n",
        "\n",
        "    sentiment_df = pd.DataFrame(sentiment_results, columns=['finbert_pos', 'finbert_neg', 'finbert_neu', 'finbert_sentiment'])\n",
        "    df = pd.concat([df, sentiment_df], axis=1)\n",
        "\n",
        "    print(\"\\nDataFrame after Sentiment Analysis:\")\n",
        "    print(df)\n",
        "\n",
        "    print(\"\\nDetailed Analysis:\")\n",
        "    for index, row in df.iterrows():\n",
        "        full_utterance = row['Utterance']\n",
        "        print(f\"Speaker: {row['Speaker']}, Utterance: {full_utterance}, Sentiment: {row['finbert_sentiment']}, Scores: (Pos: {row['finbert_pos']:.3f}, Neg: {row['finbert_neg']:.3f}, Neu: {row['finbert_neu']:.3f})\")\n",
        "\n",
        "    summary = df.groupby('Speaker').agg({\n",
        "        'finbert_pos': 'sum',\n",
        "        'finbert_neg': 'sum',\n",
        "        'finbert_neu': 'sum'\n",
        "    })\n",
        "    summary['sentiment_score'] = summary['finbert_pos'] - summary['finbert_neg']\n",
        "    print(\"\\nSummary by Speaker:\")\n",
        "    print(summary)\n",
        "\n",
        "    overall_score = summary['sentiment_score'].sum()\n",
        "    overall_sentiment = 'positive' if overall_score > 0 else 'negative'\n",
        "    print(\"\\nOverall Sentiment Report:\")\n",
        "    print(f\"Overall Score: {overall_score:.2f}\")\n",
        "    print(f\"Overall Sentiment: {overall_sentiment}\")\n",
        "\n",
        "# Example usage\n",
        "file_path = '/content/Summary (1) (1).pdf'  # Change to your file path for CSV, PDF, DOCX, or XLSX\n",
        "process_conversation(file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQxD5dxX-4i6",
        "outputId": "b9c8a634-d170-4b7d-d3e8-602cece40224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracted Dialogue DataFrame:\n",
            "Empty DataFrame\n",
            "Columns: [Speaker, Utterance]\n",
            "Index: []\n",
            "\n",
            "DataFrame after Sentiment Analysis:\n",
            "Empty DataFrame\n",
            "Columns: [Speaker, Utterance, finbert_pos, finbert_neg, finbert_neu, finbert_sentiment]\n",
            "Index: []\n",
            "\n",
            "Detailed Analysis:\n",
            "\n",
            "Summary by Speaker:\n",
            "Empty DataFrame\n",
            "Columns: [finbert_pos, finbert_neg, finbert_neu, sentiment_score]\n",
            "Index: []\n",
            "\n",
            "Overall Sentiment Report:\n",
            "Overall Score: 0.00\n",
            "Overall Sentiment: negative\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install azure-ai-textanalytics\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sphOIlhJAz6W",
        "outputId": "b6a28de9-fcbb-411d-979c-059f50c05a9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting azure-ai-textanalytics\n",
            "  Downloading azure_ai_textanalytics-5.3.0-py3-none-any.whl.metadata (82 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/82.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-core<2.0.0,>=1.24.0 (from azure-ai-textanalytics)\n",
            "  Downloading azure_core-1.31.0-py3-none-any.whl.metadata (39 kB)\n",
            "Collecting azure-common~=1.1 (from azure-ai-textanalytics)\n",
            "  Downloading azure_common-1.1.28-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting isodate<1.0.0,>=0.6.1 (from azure-ai-textanalytics)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from azure-ai-textanalytics) (4.12.2)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2.32.3)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (2024.8.30)\n",
            "Downloading azure_ai_textanalytics-5.3.0-py3-none-any.whl (298 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.6/298.6 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
            "Downloading azure_core-1.31.0-py3-none-any.whl (197 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.4/197.4 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: azure-common, isodate, azure-core, azure-ai-textanalytics\n",
            "Successfully installed azure-ai-textanalytics-5.3.0 azure-common-1.1.28 azure-core-1.31.0 isodate-0.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import PyPDF2\n",
        "import docx\n",
        "import os\n",
        "from azure.ai.textanalytics import TextAnalyticsClient\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "# Azure setup\n",
        "AZURE_ENDPOINT = \"https://hexavarsity-secureapi.azurewebsite.net/api/azureai\"  # Replace with your Azure endpoint\n",
        "AZURE_KEY = \"cc451d63b22a346c\"  # Replace with your Azure key\n",
        "\n",
        "def authenticate_client():\n",
        "    ta_credential = AzureKeyCredential(AZURE_KEY)\n",
        "    client = TextAnalyticsClient(endpoint=AZURE_ENDPOINT, credential=ta_credential)\n",
        "    return client\n",
        "\n",
        "# Azure sentiment analysis function\n",
        "def azure_sentiment(text, client):\n",
        "    documents = [text]\n",
        "    response = client.analyze_sentiment(documents=documents)[0]\n",
        "\n",
        "    sentiment = response.sentiment\n",
        "    scores = response.confidence_scores\n",
        "    return {\n",
        "        'positive': scores.positive,\n",
        "        'negative': scores.negative,\n",
        "        'neutral': scores.neutral,\n",
        "        'sentiment': sentiment\n",
        "    }\n",
        "\n",
        "# Extract conversation from different file types\n",
        "def extract_conversation_from_file(file_path):\n",
        "    file_extension = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    if file_extension == '.csv':\n",
        "        return extract_from_csv(file_path)\n",
        "    elif file_extension == '.pdf':\n",
        "        return extract_from_pdf(file_path)\n",
        "    elif file_extension == '.docx':\n",
        "        return extract_from_docx(file_path)\n",
        "    elif file_extension == '.xlsx':\n",
        "        return extract_from_excel(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please provide CSV, PDF, DOCX, or XLSX file.\")\n",
        "\n",
        "def extract_from_csv(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    if 'Speaker' in df.columns and 'Utterance' in df.columns:\n",
        "        return df[['Speaker', 'Utterance']]\n",
        "    else:\n",
        "        raise ValueError(\"CSV must contain 'Speaker' and 'Utterance' columns.\")\n",
        "\n",
        "def extract_from_pdf(file_path):\n",
        "    pdf_text = \"\"\n",
        "    with open(file_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            pdf_text += page.extract_text() + \"\\n\"\n",
        "    return pdf_text.strip()\n",
        "\n",
        "def extract_from_docx(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    doc_text = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
        "    return doc_text.strip()\n",
        "\n",
        "def extract_from_excel(file_path):\n",
        "    df = pd.read_excel(file_path)\n",
        "    if 'Speaker' in df.columns and 'Utterance' in df.columns:\n",
        "        return df[['Speaker', 'Utterance']]\n",
        "    else:\n",
        "        raise ValueError(\"Excel file must contain 'Speaker' and 'Utterance' columns.\")\n",
        "\n",
        "# Process extracted PDF conversation into a DataFrame\n",
        "def process_pdf_conversation(pdf_text):\n",
        "    lines = [line.strip() for line in pdf_text.split('\\n') if line.strip()]\n",
        "    speakers = ['Interviewer', 'Financial Expert']\n",
        "    dialogue = []\n",
        "\n",
        "    current_speaker = None\n",
        "    for line in lines:\n",
        "        for speaker in speakers:\n",
        "            if line.startswith(speaker + \":\"):\n",
        "                current_speaker = speaker\n",
        "                utterance = line[len(speaker) + 1:].strip()\n",
        "                dialogue.append((current_speaker, utterance))\n",
        "                break\n",
        "\n",
        "    df = pd.DataFrame(dialogue, columns=['Speaker', 'Utterance'])\n",
        "    return df\n",
        "\n",
        "# Perform sentiment analysis using Azure\n",
        "def perform_sentiment_analysis(df, client):\n",
        "    sentiment_results = []\n",
        "    for index, row in df.iterrows():\n",
        "        sentiment = azure_sentiment(row['Utterance'], client)\n",
        "        sentiment_results.append((\n",
        "            sentiment['positive'],\n",
        "            sentiment['negative'],\n",
        "            sentiment['neutral'],\n",
        "            sentiment['sentiment']\n",
        "        ))\n",
        "\n",
        "    sentiment_df = pd.DataFrame(sentiment_results, columns=['azure_pos', 'azure_neg', 'azure_neu', 'azure_sentiment'])\n",
        "    df = pd.concat([df, sentiment_df], axis=1)\n",
        "\n",
        "    print(\"\\nDataFrame after Sentiment Analysis:\")\n",
        "    print(df)\n",
        "\n",
        "    print(\"\\nDetailed Analysis:\")\n",
        "    for index, row in df.iterrows():\n",
        "        full_utterance = row['Utterance']\n",
        "        print(f\"Speaker: {row['Speaker']}, Utterance: {full_utterance}, Sentiment: {row['azure_sentiment']}, Scores: (Pos: {row['azure_pos']:.3f}, Neg: {row['azure_neg']:.3f}, Neu: {row['azure_neu']:.3f})\")\n",
        "\n",
        "    summary = df.groupby('Speaker').agg({\n",
        "        'azure_pos': 'sum',\n",
        "        'azure_neg': 'sum',\n",
        "        'azure_neu': 'sum'\n",
        "    })\n",
        "    summary['sentiment_score'] = summary['azure_pos'] - summary['azure_neg']\n",
        "    print(\"\\nSummary by Speaker:\")\n",
        "    print(summary)\n",
        "\n",
        "    overall_score = summary['sentiment_score'].sum()\n",
        "    overall_sentiment = 'positive' if overall_score > 0 else 'negative'\n",
        "    print(\"\\nOverall Sentiment Report:\")\n",
        "    print(f\"Overall Score: {overall_score:.2f}\")\n",
        "    print(f\"Overall Sentiment: {overall_sentiment}\")\n",
        "\n",
        "# Main function to process the conversation\n",
        "def process_conversation(file_path):\n",
        "    try:\n",
        "        client = authenticate_client()\n",
        "        extracted_data = extract_conversation_from_file(file_path)\n",
        "\n",
        "        if isinstance(extracted_data, pd.DataFrame):\n",
        "            print(\"Initial DataFrame:\")\n",
        "            print(extracted_data.head())\n",
        "            perform_sentiment_analysis(extracted_data, client)\n",
        "        else:\n",
        "            df = process_pdf_conversation(extracted_data)\n",
        "            print(\"\\nExtracted Dialogue DataFrame:\")\n",
        "            print(df)\n",
        "\n",
        "            perform_sentiment_analysis(df, client)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing the file: {e}\")\n",
        "\n",
        "# Example usage\n",
        "file_path = '/content/Document4.pdf'  # Change to your file path for CSV, PDF, DOCX, or XLSX\n",
        "process_conversation(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjve_cbMA045",
        "outputId": "73d836b4-8596-408b-efca-584c4861ba74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracted Dialogue DataFrame:\n",
            "            Speaker                                          Utterance\n",
            "0       Interviewer  Thank you for joining us today. Let's start wi...\n",
            "1  Financial Expert  Thank you for having me. Sound financial plann...\n",
            "2       Interviewer  Those are some solid principles. Moving on to ...\n",
            "3  Financial Expert  Improving your credit score involves several s...\n",
            "4       Interviewer  Thank you for that advice. Finally, what shoul...\n",
            "5  Financial Expert  When choosing a financial advisor, it's import...\n",
            "6       Interviewer  Thank you so much for sharing your expertise w...\n",
            "7  Financial Expert      You're welcome! It was a pleasure to be here.\n",
            "Error processing the file: <urllib3.connection.HTTPSConnection object at 0x7bb36f112830>: Failed to resolve 'hexavarsity-secureapi.azurewebsite.net' ([Errno -2] Name or service not known)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import PyPDF2\n",
        "import docx\n",
        "import os\n",
        "import requests\n",
        "\n",
        "# Azure OpenAI API Setup\n",
        "AZURE_OPENAI_API_KEY = \"cc451d63b22a346c\"\n",
        "AZURE_OPENAI_ENDPOINT = \"https://hexavarsity-secureapi.azurewebsite.net/api/azureai\"\n",
        "AZURE_OPENAI_API_VERSION = \"2024-02-01\"\n",
        "\n",
        "# Headers for the Azure OpenAI API request\n",
        "headers = {\n",
        "    'Content-Type': 'application/json',\n",
        "    'api-key': AZURE_OPENAI_API_KEY,\n",
        "    'api-version': AZURE_OPENAI_API_VERSION\n",
        "}\n",
        "\n",
        "# Function to call Azure OpenAI sentiment analysis\n",
        "def azure_openai_sentiment(text):\n",
        "    data = {\n",
        "        \"documents\": [\n",
        "            {\n",
        "                \"id\": \"1\",\n",
        "                \"text\": text\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    response = requests.post(f\"{AZURE_OPENAI_ENDPOINT}/analyze\", json=data, headers=headers)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        response_json = response.json()\n",
        "        sentiment = response_json['documents'][0]['sentiment']\n",
        "        confidence_scores = response_json['documents'][0]['confidenceScores']\n",
        "        return {\n",
        "            'positive': confidence_scores['positive'],\n",
        "            'negative': confidence_scores['negative'],\n",
        "            'neutral': confidence_scores['neutral'],\n",
        "            'sentiment': sentiment\n",
        "        }\n",
        "    else:\n",
        "        raise Exception(f\"API request failed with status code {response.status_code}: {response.text}\")\n",
        "\n",
        "# Extract conversation from different file types\n",
        "def extract_conversation_from_file(file_path):\n",
        "    file_extension = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    if file_extension == '.csv':\n",
        "        return extract_from_csv(file_path)\n",
        "    elif file_extension == '.pdf':\n",
        "        return extract_from_pdf(file_path)\n",
        "    elif file_extension == '.docx':\n",
        "        return extract_from_docx(file_path)\n",
        "    elif file_extension == '.xlsx':\n",
        "        return extract_from_excel(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please provide CSV, PDF, DOCX, or XLSX file.\")\n",
        "\n",
        "def extract_from_csv(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    if 'Speaker' in df.columns and 'Utterance' in df.columns:\n",
        "        return df[['Speaker', 'Utterance']]\n",
        "    else:\n",
        "        raise ValueError(\"CSV must contain 'Speaker' and 'Utterance' columns.\")\n",
        "\n",
        "def extract_from_pdf(file_path):\n",
        "    pdf_text = \"\"\n",
        "    with open(file_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            pdf_text += page.extract_text() + \"\\n\"\n",
        "    return pdf_text.strip()\n",
        "\n",
        "def extract_from_docx(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    doc_text = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
        "    return doc_text.strip()\n",
        "\n",
        "def extract_from_excel(file_path):\n",
        "    df = pd.read_excel(file_path)\n",
        "    if 'Speaker' in df.columns and 'Utterance' in df.columns:\n",
        "        return df[['Speaker', 'Utterance']]\n",
        "    else:\n",
        "        raise ValueError(\"Excel file must contain 'Speaker' and 'Utterance' columns.\")\n",
        "\n",
        "# Process extracted PDF conversation into a DataFrame\n",
        "def process_pdf_conversation(pdf_text):\n",
        "    lines = [line.strip() for line in pdf_text.split('\\n') if line.strip()]\n",
        "    speakers = ['Interviewer', 'Financial Expert']\n",
        "    dialogue = []\n",
        "\n",
        "    current_speaker = None\n",
        "    for line in lines:\n",
        "        for speaker in speakers:\n",
        "            if line.startswith(speaker + \":\"):\n",
        "                current_speaker = speaker\n",
        "                utterance = line[len(speaker) + 1:].strip()\n",
        "                dialogue.append((current_speaker, utterance))\n",
        "                break\n",
        "\n",
        "    df = pd.DataFrame(dialogue, columns=['Speaker', 'Utterance'])\n",
        "    return df\n",
        "\n",
        "# Perform sentiment analysis using Azure OpenAI API\n",
        "def perform_sentiment_analysis(df):\n",
        "    sentiment_results = []\n",
        "    for index, row in df.iterrows():\n",
        "        sentiment = azure_openai_sentiment(row['Utterance'])\n",
        "        sentiment_results.append((\n",
        "            sentiment['positive'],\n",
        "            sentiment['negative'],\n",
        "            sentiment['neutral'],\n",
        "            sentiment['sentiment']\n",
        "        ))\n",
        "\n",
        "    sentiment_df = pd.DataFrame(sentiment_results, columns=['azure_pos', 'azure_neg', 'azure_neu', 'azure_sentiment'])\n",
        "    df = pd.concat([df, sentiment_df], axis=1)\n",
        "\n",
        "    print(\"\\nDataFrame after Sentiment Analysis:\")\n",
        "    print(df)\n",
        "\n",
        "    print(\"\\nDetailed Analysis:\")\n",
        "    for index, row in df.iterrows():\n",
        "        full_utterance = row['Utterance']\n",
        "        print(f\"Speaker: {row['Speaker']}, Utterance: {full_utterance}, Sentiment: {row['azure_sentiment']}, Scores: (Pos: {row['azure_pos']:.3f}, Neg: {row['azure_neg']:.3f}, Neu: {row['azure_neu']:.3f})\")\n",
        "\n",
        "    summary = df.groupby('Speaker').agg({\n",
        "        'azure_pos': 'sum',\n",
        "        'azure_neg': 'sum',\n",
        "        'azure_neu': 'sum'\n",
        "    })\n",
        "    summary['sentiment_score'] = summary['azure_pos'] - summary['azure_neg']\n",
        "    print(\"\\nSummary by Speaker:\")\n",
        "    print(summary)\n",
        "\n",
        "    overall_score = summary['sentiment_score'].sum()\n",
        "    overall_sentiment = 'positive' if overall_score > 0 else 'negative'\n",
        "    print(\"\\nOverall Sentiment Report:\")\n",
        "    print(f\"Overall Score: {overall_score:.2f}\")\n",
        "    print(f\"Overall Sentiment: {overall_sentiment}\")\n",
        "\n",
        "# Main function to process the conversation\n",
        "def process_conversation(file_path):\n",
        "    try:\n",
        "        extracted_data = extract_conversation_from_file(file_path)\n",
        "\n",
        "        if isinstance(extracted_data, pd.DataFrame):\n",
        "            print(\"Initial DataFrame:\")\n",
        "            print(extracted_data.head())\n",
        "            perform_sentiment_analysis(extracted_data)\n",
        "        else:\n",
        "            df = process_pdf_conversation(extracted_data)\n",
        "            print(\"\\nExtracted Dialogue DataFrame:\")\n",
        "            print(df)\n",
        "\n",
        "            perform_sentiment_analysis(df)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing the file: {e}\")\n",
        "\n",
        "# Example usage\n",
        "file_path = '/content/Document3.pdf'  # Change to your file path for CSV, PDF, DOCX, or XLSX\n",
        "process_conversation(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtWNLF_MIgPw",
        "outputId": "d4eddb81-001e-43d6-d35a-9e75b6cea993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracted Dialogue DataFrame:\n",
            "            Speaker                                          Utterance\n",
            "0       Interviewer  Thank you for joining us today. Let's start wi...\n",
            "1  Financial Expert  Thank you for having me. One of the most commo...\n",
            "2       Interviewer  Those are important points. Moving on to the s...\n",
            "3  Financial Expert  Excessive debt can be extremely dangerous for ...\n",
            "4       Interviewer  That sounds quite alarming. Finally, what are ...\n",
            "5  Financial Expert  One major pitfall is underestimating how much ...\n",
            "6       Interviewer  Thank you for shedding light on these critical...\n",
            "7  Financial Expert  You're welcome. It's crucial to approach finan...\n",
            "Error processing the file: HTTPSConnectionPool(host='hexavarsity-secureapi.azurewebsite.net', port=443): Max retries exceeded with url: /api/azureai/analyze (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7bb3707bf220>: Failed to resolve 'hexavarsity-secureapi.azurewebsite.net' ([Errno -2] Name or service not known)\"))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4v5jSFicYrK",
        "outputId": "2a56732f-418f-4a6f-b77d-d34e688a954e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (1.1.2)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import PyPDF2\n",
        "import docx\n",
        "import os\n",
        "import requests\n",
        "\n",
        "# Azure OpenAI API Setup (Using your custom API keys and endpoint)\n",
        "AZURE_OPENAI_API_KEY = \"cc451d63b22a346c\"\n",
        "AZURE_OPENAI_ENDPOINT = \"https://hexavarsity-secureapi.azurewebsite.net/api/azureai\"\n",
        "AZURE_OPENAI_API_VERSION = \"2024-02-01\"\n",
        "\n",
        "# Headers for the Azure OpenAI API request\n",
        "headers = {\n",
        "    'Content-Type': 'application/json',\n",
        "    'api-key': AZURE_OPENAI_API_KEY,\n",
        "    'api-version': AZURE_OPENAI_API_VERSION\n",
        "}\n",
        "\n",
        "# Function to call custom Azure OpenAI endpoint for sentiment analysis\n",
        "def azure_openai_sentiment(text):\n",
        "    data = {\n",
        "        \"documents\": [\n",
        "            {\n",
        "                \"id\": \"1\",\n",
        "                \"text\": text\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    response = requests.post(f\"{AZURE_OPENAI_ENDPOINT}/analyze\", json=data, headers=headers)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        response_json = response.json()\n",
        "        sentiment = response_json['documents'][0]['sentiment']\n",
        "        confidence_scores = response_json['documents'][0]['confidenceScores']\n",
        "        return {\n",
        "            'positive': confidence_scores['positive'],\n",
        "            'negative': confidence_scores['negative'],\n",
        "            'neutral': confidence_scores['neutral'],\n",
        "            'sentiment': sentiment\n",
        "        }\n",
        "    else:\n",
        "        raise Exception(f\"API request failed with status code {response.status_code}: {response.text}\")\n",
        "\n",
        "# Extract conversation from different file types\n",
        "def extract_conversation_from_file(file_path):\n",
        "    file_extension = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    if file_extension == '.csv':\n",
        "        return extract_from_csv(file_path)\n",
        "    elif file_extension == '.pdf':\n",
        "        return extract_from_pdf(file_path)\n",
        "    elif file_extension == '.docx':\n",
        "        return extract_from_docx(file_path)\n",
        "    elif file_extension == '.xlsx':\n",
        "        return extract_from_excel(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please provide CSV, PDF, DOCX, or XLSX file.\")\n",
        "\n",
        "def extract_from_csv(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    if 'Speaker' in df.columns and 'Utterance' in df.columns:\n",
        "        return df[['Speaker', 'Utterance']]\n",
        "    else:\n",
        "        raise ValueError(\"CSV must contain 'Speaker' and 'Utterance' columns.\")\n",
        "\n",
        "def extract_from_pdf(file_path):\n",
        "    pdf_text = \"\"\n",
        "    with open(file_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            pdf_text += page.extract_text() + \"\\n\"\n",
        "    return pdf_text.strip()\n",
        "\n",
        "def extract_from_docx(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    doc_text = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
        "    return doc_text.strip()\n",
        "\n",
        "def extract_from_excel(file_path):\n",
        "    df = pd.read_excel(file_path)\n",
        "    if 'Speaker' in df.columns and 'Utterance' in df.columns:\n",
        "        return df[['Speaker', 'Utterance']]\n",
        "    else:\n",
        "        raise ValueError(\"Excel file must contain 'Speaker' and 'Utterance' columns.\")\n",
        "\n",
        "# Process extracted PDF conversation into a DataFrame\n",
        "def process_pdf_conversation(pdf_text):\n",
        "    lines = [line.strip() for line in pdf_text.split('\\n') if line.strip()]\n",
        "    speakers = ['Interviewer', 'Financial Expert']\n",
        "    dialogue = []\n",
        "\n",
        "    current_speaker = None\n",
        "    for line in lines:\n",
        "        for speaker in speakers:\n",
        "            if line.startswith(speaker + \":\"):\n",
        "                current_speaker = speaker\n",
        "                utterance = line[len(speaker) + 1:].strip()\n",
        "                dialogue.append((current_speaker, utterance))\n",
        "                break\n",
        "\n",
        "    df = pd.DataFrame(dialogue, columns=['Speaker', 'Utterance'])\n",
        "    return df\n",
        "\n",
        "# Perform sentiment analysis using Azure OpenAI API\n",
        "def perform_sentiment_analysis(df):\n",
        "    sentiment_results = []\n",
        "    for index, row in df.iterrows():\n",
        "        sentiment = azure_openai_sentiment(row['Utterance'])\n",
        "        sentiment_results.append((\n",
        "            sentiment['positive'],\n",
        "            sentiment['negative'],\n",
        "            sentiment['neutral'],\n",
        "            sentiment['sentiment']\n",
        "        ))\n",
        "\n",
        "    sentiment_df = pd.DataFrame(sentiment_results, columns=['azure_pos', 'azure_neg', 'azure_neu', 'azure_sentiment'])\n",
        "    df = pd.concat([df, sentiment_df], axis=1)\n",
        "\n",
        "    print(\"\\nDataFrame after Sentiment Analysis:\")\n",
        "    print(df)\n",
        "\n",
        "    print(\"\\nDetailed Analysis:\")\n",
        "    for index, row in df.iterrows():\n",
        "        full_utterance = row['Utterance']\n",
        "        print(f\"Speaker: {row['Speaker']}, Utterance: {full_utterance}, Sentiment: {row['azure_sentiment']}, Scores: (Pos: {row['azure_pos']:.3f}, Neg: {row['azure_neg']:.3f}, Neu: {row['azure_neu']:.3f})\")\n",
        "\n",
        "    summary = df.groupby('Speaker').agg({\n",
        "        'azure_pos': 'sum',\n",
        "        'azure_neg': 'sum',\n",
        "        'azure_neu': 'sum'\n",
        "    })\n",
        "    summary['sentiment_score'] = summary['azure_pos'] - summary['azure_neg']\n",
        "    print(\"\\nSummary by Speaker:\")\n",
        "    print(summary)\n",
        "\n",
        "    overall_score = summary['sentiment_score'].sum()\n",
        "    overall_sentiment = 'positive' if overall_score > 0 else 'negative'\n",
        "    print(\"\\nOverall Sentiment Report:\")\n",
        "    print(f\"Overall Score: {overall_score:.2f}\")\n",
        "    print(f\"Overall Sentiment: {overall_sentiment}\")\n",
        "\n",
        "# Main function to process the conversation\n",
        "def process_conversation(file_path):\n",
        "    try:\n",
        "        extracted_data = extract_conversation_from_file(file_path)\n",
        "\n",
        "        if isinstance(extracted_data, pd.DataFrame):\n",
        "            print(\"Initial DataFrame:\")\n",
        "            print(extracted_data.head())\n",
        "            perform_sentiment_analysis(extracted_data)\n",
        "        else:\n",
        "            df = process_pdf_conversation(extracted_data)\n",
        "            print(\"\\nExtracted Dialogue DataFrame:\")\n",
        "            print(df)\n",
        "\n",
        "            perform_sentiment_analysis(df)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing the file: {e}\")\n",
        "\n",
        "# Example usage\n",
        "file_path = '//content/Trancription.csv'  # Change to your file path for CSV, PDF, DOCX, or XLSX\n",
        "process_conversation(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozYYvb6BcNcl",
        "outputId": "4d6ce56f-a7ec-43f2-ba80-82def07f286f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial DataFrame:\n",
            "   Speaker                                          Utterance\n",
            "0  Sandeep   One thing which I've taken away for the wealt...\n",
            "1  Ranveer   People are running out of money because of ba...\n",
            "2  Sandeep   No, they're in fact, they will live so long. ...\n",
            "3  Ranveer   What do you think Adani and Mukesham money do...\n",
            "4  Ranveer   Now there's countless content pieces on YouTu...\n",
            "Error processing the file: HTTPSConnectionPool(host='hexavarsity-secureapi.azurewebsite.net', port=443): Max retries exceeded with url: /api/azureai/analyze (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x791aa8d58a30>: Failed to resolve 'hexavarsity-secureapi.azurewebsite.net' ([Errno -2] Name or service not known)\"))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import PyPDF2\n",
        "import docx\n",
        "\n",
        "# Azure Cognitive Services API Setup\n",
        "AZURE_TEXT_ANALYTICS_KEY = \"your_azure_text_analytics_key\"  # Replace with your actual key\n",
        "AZURE_TEXT_ANALYTICS_ENDPOINT = \"https://your-cognitive-services-endpoint.cognitiveservices.azure.com/\"  # Replace with your endpoint\n",
        "\n",
        "# Headers for the Azure Text Analytics API request\n",
        "headers = {\n",
        "    'Ocp-Apim-Subscription-Key': AZURE_TEXT_ANALYTICS_KEY,\n",
        "    'Content-Type': 'application/json'\n",
        "}\n",
        "\n",
        "# Function to call Azure Text Analytics endpoint for sentiment analysis\n",
        "def azure_sentiment_analysis(text):\n",
        "    url = f\"{AZURE_TEXT_ANALYTICS_ENDPOINT}/text/analytics/v3.1/sentiment\"\n",
        "\n",
        "    data = {\n",
        "        \"documents\": [\n",
        "            {\n",
        "                \"id\": \"1\",\n",
        "                \"language\": \"en\",\n",
        "                \"text\": text\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        response_json = response.json()\n",
        "        sentiment = response_json['documents'][0]['sentiment']\n",
        "        confidence_scores = response_json['documents'][0]['confidenceScores']\n",
        "        return {\n",
        "            'positive': confidence_scores['positive'],\n",
        "            'negative': confidence_scores['negative'],\n",
        "            'neutral': confidence_scores['neutral'],\n",
        "            'sentiment': sentiment\n",
        "        }\n",
        "    else:\n",
        "        raise Exception(f\"API request failed with status code {response.status_code}: {response.text}\")\n",
        "\n",
        "# Extract conversation from different file types\n",
        "def extract_conversation_from_file(file_path):\n",
        "    file_extension = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    if file_extension == '.csv':\n",
        "        return extract_from_csv(file_path)\n",
        "    elif file_extension == '.pdf':\n",
        "        return extract_from_pdf(file_path)\n",
        "    elif file_extension == '.docx':\n",
        "        return extract_from_docx(file_path)\n",
        "    elif file_extension == '.xlsx':\n",
        "        return extract_from_excel(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please provide CSV, PDF, DOCX, or XLSX file.\")\n",
        "\n",
        "def extract_from_csv(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    if 'Speaker' in df.columns and 'Utterance' in df.columns:\n",
        "        return df[['Speaker', 'Utterance']]\n",
        "    else:\n",
        "        raise ValueError(\"CSV must contain 'Speaker' and 'Utterance' columns.\")\n",
        "\n",
        "def extract_from_pdf(file_path):\n",
        "    pdf_text = \"\"\n",
        "    with open(file_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            pdf_text += page.extract_text() + \"\\n\"\n",
        "    return pdf_text.strip()\n",
        "\n",
        "def extract_from_docx(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    doc_text = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
        "    return doc_text.strip()\n",
        "\n",
        "def extract_from_excel(file_path):\n",
        "    df = pd.read_excel(file_path)\n",
        "    if 'Speaker' in df.columns and 'Utterance' in df.columns:\n",
        "        return df[['Speaker', 'Utterance']]\n",
        "    else:\n",
        "        raise ValueError(\"Excel file must contain 'Speaker' and 'Utterance' columns.\")\n",
        "\n",
        "# Process extracted PDF conversation into a DataFrame\n",
        "def process_pdf_conversation(pdf_text):\n",
        "    lines = [line.strip() for line in pdf_text.split('\\n') if line.strip()]\n",
        "    speakers = ['Interviewer', 'Financial Expert']\n",
        "    dialogue = []\n",
        "\n",
        "    current_speaker = None\n",
        "    for line in lines:\n",
        "        for speaker in speakers:\n",
        "            if line.startswith(speaker + \":\"):\n",
        "                current_speaker = speaker\n",
        "                utterance = line[len(speaker) + 1:].strip()\n",
        "                dialogue.append((current_speaker, utterance))\n",
        "                break\n",
        "\n",
        "    df = pd.DataFrame(dialogue, columns=['Speaker', 'Utterance'])\n",
        "    return df\n",
        "\n",
        "# Perform sentiment analysis using Azure Text Analytics API\n",
        "def perform_sentiment_analysis(df):\n",
        "    sentiment_results = []\n",
        "    for index, row in df.iterrows():\n",
        "        sentiment = azure_sentiment_analysis(row['Utterance'])\n",
        "        sentiment_results.append((\n",
        "            sentiment['positive'],\n",
        "            sentiment['negative'],\n",
        "            sentiment['neutral'],\n",
        "            sentiment['sentiment']\n",
        "        ))\n",
        "\n",
        "    sentiment_df = pd.DataFrame(sentiment_results, columns=['azure_pos', 'azure_neg', 'azure_neu', 'azure_sentiment'])\n",
        "    df = pd.concat([df, sentiment_df], axis=1)\n",
        "\n",
        "    print(\"\\nDataFrame after Sentiment Analysis:\")\n",
        "    print(df)\n",
        "\n",
        "    print(\"\\nDetailed Analysis:\")\n",
        "    for index, row in df.iterrows():\n",
        "        full_utterance = row['Utterance']\n",
        "        print(f\"Speaker: {row['Speaker']}, Utterance: {full_utterance}, Sentiment: {row['azure_sentiment']}, Scores: (Pos: {row['azure_pos']:.3f}, Neg: {row['azure_neg']:.3f}, Neu: {row['azure_neu']:.3f})\")\n",
        "\n",
        "    summary = df.groupby('Speaker').agg({\n",
        "        'azure_pos': 'sum',\n",
        "        'azure_neg': 'sum',\n",
        "        'azure_neu': 'sum'\n",
        "    })\n",
        "    summary['sentiment_score'] = summary['azure_pos'] - summary['azure_neg']\n",
        "    print(\"\\nSummary by Speaker:\")\n",
        "    print(summary)\n",
        "\n",
        "    overall_score = summary['sentiment_score'].sum()\n",
        "    overall_sentiment = 'positive' if overall_score > 0 else 'negative'\n",
        "    print(\"\\nOverall Sentiment Report:\")\n",
        "    print(f\"Overall Score: {overall_score:.2f}\")\n",
        "    print(f\"Overall Sentiment: {overall_sentiment}\")\n",
        "\n",
        "# Main function to process the conversation\n",
        "def process_conversation(file_path):\n",
        "    try:\n",
        "        extracted_data = extract_conversation_from_file(file_path)\n",
        "\n",
        "        if isinstance(extracted_data, pd.DataFrame):\n",
        "            print(\"Initial DataFrame:\")\n",
        "            print(extracted_data.head())\n",
        "            perform_sentiment_analysis(extracted_data)\n",
        "        else:\n",
        "            df = process_pdf_conversation(extracted_data)\n",
        "            print(\"\\nExtracted Dialogue DataFrame:\")\n",
        "            print(df)\n",
        "\n",
        "            perform_sentiment_analysis(df)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing the file: {e}\")\n",
        "\n",
        "# Example usage\n",
        "file_path = '/content/Trancription.csv'  # Change to your file path for CSV, PDF, DOCX, or XLSX\n",
        "process_conversation(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AZK8qiidn58",
        "outputId": "289ac790-c1e4-448a-be85-d04cc437dd7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial DataFrame:\n",
            "   Speaker                                          Utterance\n",
            "0  Sandeep   One thing which I've taken away for the wealt...\n",
            "1  Ranveer   People are running out of money because of ba...\n",
            "2  Sandeep   No, they're in fact, they will live so long. ...\n",
            "3  Ranveer   What do you think Adani and Mukesham money do...\n",
            "4  Ranveer   Now there's countless content pieces on YouTu...\n",
            "Error processing the file: HTTPSConnectionPool(host='your-cognitive-services-endpoint.cognitiveservices.azure.com', port=443): Max retries exceeded with url: /text/analytics/v3.1/sentiment (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x791aa8d58eb0>: Failed to resolve 'your-cognitive-services-endpoint.cognitiveservices.azure.com' ([Errno -2] Name or service not known)\"))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip uninstall torch\n",
        "pip install torch torchvision torchaudio\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "WYVDkYKFf7GV",
        "outputId": "54231c2a-9cd7-4082-afef-8d79a1e8d486"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-29-d3596db05164>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-29-d3596db05164>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip uninstall torch\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import PyPDF2\n",
        "import docx\n",
        "import os\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load Hugging Face sentiment analysis pipeline\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "def huggingface_sentiment(text):\n",
        "    # Perform sentiment analysis on the text\n",
        "    result = sentiment_pipeline(text)[0]\n",
        "    label = result['label']\n",
        "    score = result['score']\n",
        "\n",
        "    # Map Hugging Face labels to more user-friendly labels\n",
        "    if label == 'LABEL_2':  # Positive\n",
        "        sentiment = 'positive'\n",
        "    elif label == 'LABEL_1':  # Neutral\n",
        "        sentiment = 'neutral'\n",
        "    else:  # Negative\n",
        "        sentiment = 'negative'\n",
        "\n",
        "    return {\n",
        "        'sentiment': sentiment,\n",
        "        'score': score\n",
        "    }\n",
        "\n",
        "def extract_conversation_from_file(file_path):\n",
        "    file_extension = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    if file_extension == '.csv':\n",
        "        return extract_from_csv(file_path)\n",
        "    elif file_extension == '.pdf':\n",
        "        return extract_from_pdf(file_path)\n",
        "    elif file_extension == '.docx':\n",
        "        return extract_from_docx(file_path)\n",
        "    elif file_extension == '.xlsx':\n",
        "        return extract_from_excel(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please provide CSV, PDF, DOCX, or XLSX file.\")\n",
        "\n",
        "def extract_from_csv(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    if 'Speaker' in df.columns and 'Utterance' in df.columns:\n",
        "        return df[['Speaker', 'Utterance']]\n",
        "    else:\n",
        "        raise ValueError(\"CSV must contain 'Speaker' and 'Utterance' columns.\")\n",
        "\n",
        "def extract_from_pdf(file_path):\n",
        "    pdf_text = \"\"\n",
        "    with open(file_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            pdf_text += page.extract_text() + \"\\n\"\n",
        "    return pdf_text.strip()\n",
        "\n",
        "def extract_from_docx(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    doc_text = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
        "    return doc_text.strip()\n",
        "\n",
        "def extract_from_excel(file_path):\n",
        "    df = pd.read_excel(file_path)\n",
        "    if 'Speaker' in df.columns and 'Utterance' in df.columns:\n",
        "        return df[['Speaker', 'Utterance']]\n",
        "    else:\n",
        "        raise ValueError(\"Excel file must contain 'Speaker' and 'Utterance' columns.\")\n",
        "\n",
        "def process_pdf_conversation(pdf_text):\n",
        "    lines = [line.strip() for line in pdf_text.split('\\n') if line.strip()]\n",
        "    speakers = ['Interviewer', 'Financial Expert']\n",
        "    dialogue = []\n",
        "\n",
        "    current_speaker = None\n",
        "    for line in lines:\n",
        "        for speaker in speakers:\n",
        "            if line.startswith(speaker + \":\"):\n",
        "                current_speaker = speaker\n",
        "                utterance = line[len(speaker) + 1:].strip()\n",
        "                dialogue.append((current_speaker, utterance))\n",
        "                break\n",
        "\n",
        "    df = pd.DataFrame(dialogue, columns=['Speaker', 'Utterance'])\n",
        "    return df\n",
        "\n",
        "def process_conversation(file_path):\n",
        "    try:\n",
        "        extracted_data = extract_conversation_from_file(file_path)\n",
        "\n",
        "        if isinstance(extracted_data, pd.DataFrame):\n",
        "            print(\"Initial DataFrame:\")\n",
        "            print(extracted_data.head())\n",
        "            perform_sentiment_analysis(extracted_data)\n",
        "        else:\n",
        "            df = process_pdf_conversation(extracted_data)\n",
        "            print(\"\\nExtracted Dialogue DataFrame:\")\n",
        "            print(df)\n",
        "            perform_sentiment_analysis(df)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing the file: {e}\")\n",
        "\n",
        "def perform_sentiment_analysis(df):\n",
        "    sentiment_results = []\n",
        "    for index, row in df.iterrows():\n",
        "        sentiment = huggingface_sentiment(row['Utterance'])\n",
        "        sentiment_results.append((\n",
        "            sentiment['score'],\n",
        "            sentiment['sentiment']\n",
        "        ))\n",
        "\n",
        "    sentiment_df = pd.DataFrame(sentiment_results, columns=['sentiment_score', 'sentiment'])\n",
        "    df = pd.concat([df, sentiment_df], axis=1)\n",
        "\n",
        "    print(\"\\nDataFrame after Sentiment Analysis:\")\n",
        "    print(df)\n",
        "\n",
        "    print(\"\\nDetailed Analysis:\")\n",
        "    for index, row in df.iterrows():\n",
        "        full_utterance = row['Utterance']\n",
        "        print(f\"Speaker: {row['Speaker']}, Utterance: {full_utterance}, Sentiment: {row['sentiment']}, Score: {row['sentiment_score']:.3f}\")\n",
        "\n",
        "    summary = df.groupby('Speaker').agg({\n",
        "        'sentiment_score': 'mean'\n",
        "    })\n",
        "    print(\"\\nSummary by Speaker:\")\n",
        "    print(summary)\n",
        "\n",
        "    overall_score = summary['sentiment_score'].mean()\n",
        "    overall_sentiment = 'positive' if overall_score > 0.5 else 'negative'\n",
        "    print(\"\\nOverall Sentiment Report:\")\n",
        "    print(f\"Overall Score: {overall_score:.2f}\")\n",
        "    print(f\"Overall Sentiment: {overall_sentiment}\")\n",
        "\n",
        "# Example usage\n",
        "file_path = '/content/Trancription.csv'  # Replace with the actual path to your file\n",
        "process_conversation(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "cwN9PUWafTDc",
        "outputId": "03371fa3-6d4b-42ef-de82-bf995c1d6600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "THPDtypeType.tp_dict == nullptr INTERNAL ASSERT FAILED at \"../torch/csrc/Dtype.cpp\":176, please report a bug to PyTorch. ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-12734e0cb03c>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdocx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load Hugging Face sentiment analysis pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m from .utils import (\n\u001b[1;32m     28\u001b[0m     \u001b[0mOptionalDependencyNotAvailable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdependency_versions_table\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_version_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mreplace_return_docstrings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m )\n\u001b[0;32m---> 34\u001b[0;31m from .generic import (\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mContextManagers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mExplicitEnum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pytree\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_torch_pytree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_model_output_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModelOutput\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_torch_pytree.Context\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mUSE_GLOBAL_DEPS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0m_load_global_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;31m# Appease the type checker; ordinarily this binding is inserted by the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: THPDtypeType.tp_dict == nullptr INTERNAL ASSERT FAILED at \"../torch/csrc/Dtype.cpp\":176, please report a bug to PyTorch. "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required libraries (run this in your Colab notebook)\n",
        "#!pip install pandas PyPDF2 python-docx transformers torch\n",
        "\n",
        "import pandas as pd\n",
        "import PyPDF2\n",
        "import docx\n",
        "import os\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load Hugging Face sentiment analysis pipeline with FinBERT\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"yiyanghkust/finbert-tone\")\n",
        "\n",
        "def finbert_sentiment(text):\n",
        "    result = sentiment_pipeline(text)[0]\n",
        "    sentiment_label = result['label']\n",
        "    score = result['score']\n",
        "\n",
        "    if sentiment_label == 'negative':\n",
        "        return {'positive': 0, 'negative': score, 'neutral': 0, 'sentiment': 'negative'}\n",
        "    elif sentiment_label == 'neutral':\n",
        "        return {'positive': 0, 'negative': 0, 'neutral': score, 'sentiment': 'neutral'}\n",
        "    else:  # Positive\n",
        "        return {'positive': score, 'negative': 0, 'neutral': 0, 'sentiment': 'positive'}\n",
        "\n",
        "def extract_conversation_from_file(file_path):\n",
        "    file_extension = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    if file_extension == '.csv':\n",
        "        return extract_from_csv(file_path)\n",
        "    elif file_extension == '.pdf':\n",
        "        return extract_from_pdf(file_path)\n",
        "    elif file_extension == '.docx':\n",
        "        return extract_from_docx(file_path)\n",
        "    elif file_extension == '.xlsx':\n",
        "        return extract_from_excel(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please provide CSV, PDF, DOCX, or XLSX file.\")\n",
        "\n",
        "def extract_from_csv(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    if 'Speaker' in df.columns and 'Utterance' in df.columns:\n",
        "        return df[['Speaker', 'Utterance']]\n",
        "    else:\n",
        "        raise ValueError(\"CSV must contain 'Speaker' and 'Utterance' columns.\")\n",
        "\n",
        "def extract_from_pdf(file_path):\n",
        "    pdf_text = \"\"\n",
        "    with open(file_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            pdf_text += page.extract_text() + \"\\n\"\n",
        "    return pdf_text.strip()\n",
        "\n",
        "def extract_from_docx(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    doc_text = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
        "    return doc_text.strip()\n",
        "\n",
        "def extract_from_excel(file_path):\n",
        "    df = pd.read_excel(file_path)\n",
        "    if 'Speaker' in df.columns and 'Utterance' in df.columns:\n",
        "        return df[['Speaker', 'Utterance']]\n",
        "    else:\n",
        "        raise ValueError(\"Excel file must contain 'Speaker' and 'Utterance' columns.\")\n",
        "\n",
        "def process_pdf_conversation(pdf_text):\n",
        "    lines = [line.strip() for line in pdf_text.split('\\n') if line.strip()]\n",
        "    speakers = ['Interviewer', 'Financial Expert']\n",
        "    dialogue = []\n",
        "\n",
        "    current_speaker = None\n",
        "    for line in lines:\n",
        "        for speaker in speakers:\n",
        "            if line.startswith(speaker + \":\"):\n",
        "                current_speaker = speaker\n",
        "                utterance = line[len(speaker) + 1:].strip()\n",
        "                dialogue.append((current_speaker, utterance))\n",
        "                break\n",
        "\n",
        "    df = pd.DataFrame(dialogue, columns=['Speaker', 'Utterance'])\n",
        "    return df\n",
        "\n",
        "def process_conversation(file_path):\n",
        "    try:\n",
        "        extracted_data = extract_conversation_from_file(file_path)\n",
        "\n",
        "        if isinstance(extracted_data, pd.DataFrame):\n",
        "            print(\"Initial DataFrame:\")\n",
        "            print(extracted_data.head())\n",
        "            perform_sentiment_analysis(extracted_data)\n",
        "        else:\n",
        "            df = process_pdf_conversation(extracted_data)\n",
        "            print(\"\\nExtracted Dialogue DataFrame:\")\n",
        "            print(df)\n",
        "\n",
        "            perform_sentiment_analysis(df)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing the file: {e}\")\n",
        "\n",
        "def perform_sentiment_analysis(df):\n",
        "    sentiment_results = []\n",
        "    for index, row in df.iterrows():\n",
        "        sentiment = finbert_sentiment(row['Utterance'])\n",
        "        sentiment_results.append((\n",
        "            sentiment['positive'],\n",
        "            sentiment['negative'],\n",
        "            sentiment['neutral'],\n",
        "            sentiment['sentiment']\n",
        "        ))\n",
        "\n",
        "    sentiment_df = pd.DataFrame(sentiment_results, columns=['finbert_pos', 'finbert_neg', 'finbert_neu', 'finbert_sentiment'])\n",
        "    df = pd.concat([df, sentiment_df], axis=1)\n",
        "\n",
        "    print(\"\\nDataFrame after Sentiment Analysis:\")\n",
        "    print(df)\n",
        "\n",
        "    print(\"\\nDetailed Analysis:\")\n",
        "    for index, row in df.iterrows():\n",
        "        full_utterance = row['Utterance']\n",
        "        print(f\"Speaker: {row['Speaker']}, Utterance: {full_utterance}, Sentiment: {row['finbert_sentiment']}, Scores: (Pos: {row['finbert_pos']:.3f}, Neg: {row['finbert_neg']:.3f}, Neu: {row['finbert_neu']:.3f})\")\n",
        "\n",
        "    summary = df.groupby('Speaker').agg({\n",
        "        'finbert_pos': 'sum',\n",
        "        'finbert_neg': 'sum',\n",
        "        'finbert_neu': 'sum'\n",
        "    })\n",
        "    summary['sentiment_score'] = summary['finbert_pos'] - summary['finbert_neg']\n",
        "\n",
        "    # Determine overall sentiment\n",
        "    summary['overall_sentiment'] = summary['sentiment_score'].apply(lambda x: 'positive' if x > 0 else ('negative' if x < 0 else 'neutral'))\n",
        "\n",
        "    print(\"\\nSummary by Speaker:\")\n",
        "    print(summary)\n",
        "\n",
        "    overall_score = summary['sentiment_score'].sum()\n",
        "    overall_sentiment = 'positive' if overall_score > 0 else 'negative' if overall_score < 0 else 'neutral'\n",
        "    print(\"\\nOverall Sentiment Report:\")\n",
        "    print(f\"Overall Score: {overall_score:.2f}\")\n",
        "    print(f\"Overall Sentiment: {overall_sentiment}\")\n",
        "\n",
        "# Example usage\n",
        "file_path = '/content/full_utterances (2) 1.xlsx'  # Change to your file path for CSV, PDF, DOCX, or XLSX\n",
        "process_conversation(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fC1SyBDPFV0b",
        "outputId": "6702030b-b307-4141-ecd7-6b196b9609b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (1.1.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial DataFrame:\n",
            "   Speaker                                          Utterance\n",
            "0  Sandeep   One thing which I've taken away for the wealt...\n",
            "1  Ranveer   People are running out of money because of ba...\n",
            "2  Sandeep   No, they're in fact, they will live so long. ...\n",
            "3  Ranveer   What do you think Adani and Mukesham money do...\n",
            "4  Ranveer   Now there's countless content pieces on YouTu...\n",
            "\n",
            "DataFrame after Sentiment Analysis:\n",
            "   Speaker                                          Utterance  finbert_pos  \\\n",
            "0  Sandeep   One thing which I've taken away for the wealt...     0.933711   \n",
            "1  Ranveer   People are running out of money because of ba...     0.521271   \n",
            "2  Sandeep   No, they're in fact, they will live so long. ...     0.998378   \n",
            "3  Ranveer   What do you think Adani and Mukesham money do...     0.999963   \n",
            "4  Ranveer   Now there's countless content pieces on YouTu...     0.999927   \n",
            "\n",
            "   finbert_neg  finbert_neu finbert_sentiment  \n",
            "0            0            0          positive  \n",
            "1            0            0          positive  \n",
            "2            0            0          positive  \n",
            "3            0            0          positive  \n",
            "4            0            0          positive  \n",
            "\n",
            "Detailed Analysis:\n",
            "Speaker: Sandeep, Utterance:  One thing which I've taken away for the wealthy is that they are very serious about growth. They make sure that the money is compounding over a long period of time. The US by the way, that people are running out of money in their older age., Sentiment: positive, Scores: (Pos: 0.934, Neg: 0.000, Neu: 0.000)\n",
            "Speaker: Ranveer, Utterance:  People are running out of money because of bad planning or because of inflation., Sentiment: positive, Scores: (Pos: 0.521, Neg: 0.000, Neu: 0.000)\n",
            "Speaker: Sandeep, Utterance:  No, they're in fact, they will live so long. Oh shit., Sentiment: positive, Scores: (Pos: 0.998, Neg: 0.000, Neu: 0.000)\n",
            "Speaker: Ranveer, Utterance:  What do you think Adani and Mukesham money do with their own money? And by that I mean not reliance his money, Adani groups money, their own money as men., Sentiment: positive, Scores: (Pos: 1.000, Neg: 0.000, Neu: 0.000)\n",
            "Speaker: Ranveer, Utterance:  Now there's countless content pieces on YouTube on Spotify where you can learn about money, but it's these kind of conversations with experts from the world of finance people who manage money in the range of 50,000 rupees and savings to 50 crores and savings per month. That's the experience, the financial know how of our guest today. It's Sandeep Jaitwani on the show. His startup Dizerve.in is a modern day portfolio management service. Personally for me, most of my money is handled not by myself, but by portfolio management services. I trust this man enough to give him my own money. He is handling some of beer by himself media world's funds. That's why I thought that on this episode, let me give you guys access to someone who is given me financial advice, someone that I trust my money with. It's going to be a very raw and open conversation about everything from how the richest in India manages their money to how you can manage your money even if you're just beginning your career right now. It's a financial edu key., Sentiment: positive, Scores: (Pos: 1.000, Neg: 0.000, Neu: 0.000)\n",
            "\n",
            "Summary by Speaker:\n",
            "         finbert_pos  finbert_neg  finbert_neu  sentiment_score  \\\n",
            "Speaker                                                           \n",
            "Ranveer     2.521161            0            0         2.521161   \n",
            "Sandeep     1.932090            0            0         1.932090   \n",
            "\n",
            "        overall_sentiment  \n",
            "Speaker                    \n",
            "Ranveer          positive  \n",
            "Sandeep          positive  \n",
            "\n",
            "Overall Sentiment Report:\n",
            "Overall Score: 4.45\n",
            "Overall Sentiment: positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required libraries (run this in your Colab notebook)\n",
        "#!pip install pandas PyPDF2 python-docx transformers torch\n",
        "\n",
        "import pandas as pd\n",
        "import PyPDF2\n",
        "import docx\n",
        "import os\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load Hugging Face sentiment analysis pipeline with FinBERT\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"yiyanghkust/finbert-tone\")\n",
        "\n",
        "def finbert_sentiment(text):\n",
        "    result = sentiment_pipeline(text)[0]\n",
        "    sentiment_label = result['label']\n",
        "    score = result['score']\n",
        "\n",
        "    if sentiment_label == 'negative':\n",
        "        return {'positive': 0, 'negative': score, 'neutral': 0, 'sentiment': 'negative'}\n",
        "    elif sentiment_label == 'neutral':\n",
        "        return {'positive': 0, 'negative': 0, 'neutral': score, 'sentiment': 'neutral'}\n",
        "    else:  # Positive\n",
        "        return {'positive': score, 'negative': 0, 'neutral': 0, 'sentiment': 'positive'}\n",
        "\n",
        "def extract_conversation_from_file(file_path):\n",
        "    file_extension = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    if file_extension == '.csv':\n",
        "        return extract_from_csv(file_path)\n",
        "    elif file_extension == '.pdf':\n",
        "        return extract_from_pdf(file_path)\n",
        "    elif file_extension == '.docx':\n",
        "        return extract_from_docx(file_path)\n",
        "    elif file_extension == '.xlsx':\n",
        "        return extract_from_excel(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please provide CSV, PDF, DOCX, or XLSX file.\")\n",
        "\n",
        "def extract_from_csv(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    if 'Speaker' in df.columns and 'Utterance' in df.columns:\n",
        "        return df[['Speaker', 'Utterance']]\n",
        "    else:\n",
        "        raise ValueError(\"CSV must contain 'Speaker' and 'Utterance' columns.\")\n",
        "\n",
        "def extract_from_pdf(file_path):\n",
        "    pdf_text = \"\"\n",
        "    with open(file_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            pdf_text += page.extract_text() + \"\\n\"\n",
        "    return pdf_text.strip()\n",
        "\n",
        "def extract_from_docx(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    doc_text = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
        "    return doc_text.strip()\n",
        "\n",
        "def extract_from_excel(file_path):\n",
        "    df = pd.read_excel(file_path)\n",
        "    if 'Speaker' in df.columns and 'Utterance' in df.columns:\n",
        "        return df[['Speaker', 'Utterance']]\n",
        "    else:\n",
        "        raise ValueError(\"Excel file must contain 'Speaker' and 'Utterance' columns.\")\n",
        "\n",
        "def process_pdf_conversation(pdf_text):\n",
        "    lines = [line.strip() for line in pdf_text.split('\\n') if line.strip()]\n",
        "    speakers = ['Interviewer', 'Financial Expert']\n",
        "    dialogue = []\n",
        "\n",
        "    current_speaker = None\n",
        "    for line in lines:\n",
        "        for speaker in speakers:\n",
        "            if line.startswith(speaker + \":\"):\n",
        "                current_speaker = speaker\n",
        "                utterance = line[len(speaker) + 1:].strip()\n",
        "                dialogue.append((current_speaker, utterance))\n",
        "                break\n",
        "\n",
        "    df = pd.DataFrame(dialogue, columns=['Speaker', 'Utterance'])\n",
        "    return df\n",
        "\n",
        "def process_conversation(file_path):\n",
        "    try:\n",
        "        extracted_data = extract_conversation_from_file(file_path)\n",
        "\n",
        "        if isinstance(extracted_data, pd.DataFrame):\n",
        "            print(\"Initial DataFrame:\")\n",
        "            print(extracted_data.head())\n",
        "            perform_sentiment_analysis(extracted_data)\n",
        "        else:\n",
        "            df = process_pdf_conversation(extracted_data)\n",
        "            print(\"\\nExtracted Dialogue DataFrame:\")\n",
        "            print(df)\n",
        "\n",
        "            perform_sentiment_analysis(df)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing the file: {e}\")\n",
        "\n",
        "def perform_sentiment_analysis(df):\n",
        "    sentiment_results = []\n",
        "    for index, row in df.iterrows():\n",
        "        sentiment = finbert_sentiment(row['Utterance'])\n",
        "        sentiment_results.append((\n",
        "            sentiment['positive'],\n",
        "            sentiment['negative'],\n",
        "            sentiment['neutral'],\n",
        "            sentiment['sentiment']\n",
        "        ))\n",
        "\n",
        "    sentiment_df = pd.DataFrame(sentiment_results, columns=['finbert_pos', 'finbert_neg', 'finbert_neu', 'finbert_sentiment'])\n",
        "    df = pd.concat([df, sentiment_df], axis=1)\n",
        "\n",
        "    print(\"\\nDataFrame after Sentiment Analysis:\")\n",
        "    print(df)\n",
        "\n",
        "    print(\"\\nDetailed Analysis:\")\n",
        "    for index, row in df.iterrows():\n",
        "        full_utterance = row['Utterance']\n",
        "        print(f\"Speaker: {row['Speaker']}, Utterance: {full_utterance}, Sentiment: {row['finbert_sentiment']}, Scores: (Pos: {row['finbert_pos']:.3f}, Neg: {row['finbert_neg']:.3f}, Neu: {row['finbert_neu']:.3f})\")\n",
        "\n",
        "    summary = df.groupby('Speaker').agg({\n",
        "        'finbert_pos': 'sum',\n",
        "        'finbert_neg': 'sum',\n",
        "        'finbert_neu': 'sum'\n",
        "    })\n",
        "    summary['sentiment_score'] = summary['finbert_pos'] - summary['finbert_neg']\n",
        "\n",
        "    # Normalizing the sentiment score on a scale of 1 to 5\n",
        "    # You can change the scale (e.g., 1 to 10) by adjusting the max_score\n",
        "    max_score = 5\n",
        "    summary['normalized_score'] = (summary['sentiment_score'] / summary['sentiment_score'].abs().max()) * max_score\n",
        "    summary['normalized_score'] = summary['normalized_score'].fillna(0).clip(lower=1, upper=max_score)\n",
        "\n",
        "    # Determine overall sentiment\n",
        "    summary['overall_sentiment'] = summary['sentiment_score'].apply(lambda x: 'positive' if x > 0 else ('negative' if x < 0 else 'neutral'))\n",
        "\n",
        "    print(\"\\nSummary by Speaker:\")\n",
        "    print(summary)\n",
        "\n",
        "    # Calculate the overall sentiment score\n",
        "    overall_score = summary['sentiment_score'].sum()\n",
        "    overall_sentiment = 'positive' if overall_score > 0 else 'negative' if overall_score < 0 else 'neutral'\n",
        "\n",
        "    # Normalize overall score on a scale of 1 to 5\n",
        "    normalized_overall_score = (overall_score / summary['sentiment_score'].abs().max()) * max_score\n",
        "    normalized_overall_score = max(1, min(normalized_overall_score, max_score))  # Clip to 1-5 range\n",
        "\n",
        "    print(\"\\nOverall Sentiment Report:\")\n",
        "    print(f\"Overall Normalized Score (Scale 1-{max_score}): {normalized_overall_score:.2f}\")\n",
        "    print(f\"Overall Sentiment: {overall_sentiment}\")\n",
        "\n",
        "# Example usage\n",
        "file_path = '/content/Document6.pdf'  # Change to your file path for CSV, PDF, DOCX, or XLSX\n",
        "process_conversation(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NddcmZJWI-qs",
        "outputId": "a86708a6-a3f4-4a5e-9a57-cf559e5325bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracted Dialogue DataFrame:\n",
            "Empty DataFrame\n",
            "Columns: [Speaker, Utterance]\n",
            "Index: []\n",
            "\n",
            "DataFrame after Sentiment Analysis:\n",
            "Empty DataFrame\n",
            "Columns: [Speaker, Utterance, finbert_pos, finbert_neg, finbert_neu, finbert_sentiment]\n",
            "Index: []\n",
            "\n",
            "Detailed Analysis:\n",
            "\n",
            "Summary by Speaker:\n",
            "Empty DataFrame\n",
            "Columns: [finbert_pos, finbert_neg, finbert_neu, sentiment_score, normalized_score, overall_sentiment]\n",
            "Index: []\n",
            "\n",
            "Overall Sentiment Report:\n",
            "Overall Normalized Score (Scale 1-5): 1.00\n",
            "Overall Sentiment: neutral\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required libraries (run this in your Colab notebook)\n",
        "!pip install pandas PyPDF2 python-docx transformers torch\n",
        "\n",
        "import pandas as pd\n",
        "import PyPDF2\n",
        "import docx\n",
        "import os\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load Hugging Face sentiment analysis pipeline with FinBERT\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"yiyanghkust/finbert-tone\")\n",
        "\n",
        "def finbert_sentiment(text):\n",
        "    result = sentiment_pipeline(text)[0]\n",
        "    sentiment_label = result['label']\n",
        "    score = result['score']\n",
        "\n",
        "    if sentiment_label == 'negative':\n",
        "        return {'positive': 0, 'negative': score, 'neutral': 0, 'sentiment': 'negative'}\n",
        "    elif sentiment_label == 'neutral':\n",
        "        return {'positive': 0, 'negative': 0, 'neutral': score, 'sentiment': 'neutral'}\n",
        "    else:  # Positive\n",
        "        return {'positive': score, 'negative': 0, 'neutral': 0, 'sentiment': 'positive'}\n",
        "\n",
        "# Correct PDF extraction function\n",
        "def extract_from_pdf(file_path):\n",
        "    pdf_text = \"\"\n",
        "    with open(file_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:  # Ensure non-empty text is extracted\n",
        "                pdf_text += page_text + \"\\n\"\n",
        "    return pdf_text.strip()\n",
        "\n",
        "# Correct DOCX extraction function\n",
        "def extract_from_docx(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    doc_text = '\\n'.join([paragraph.text for paragraph in doc.paragraphs if paragraph.text.strip()])\n",
        "    return doc_text.strip()\n",
        "\n",
        "def process_conversation(file_path):\n",
        "    file_extension = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    if file_extension == '.pdf':\n",
        "        conversation_text = extract_from_pdf(file_path)\n",
        "    elif file_extension == '.docx':\n",
        "        conversation_text = extract_from_docx(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please provide a PDF or DOCX file.\")\n",
        "\n",
        "    if conversation_text:\n",
        "        # Split the conversation by lines\n",
        "        lines = [line.strip() for line in conversation_text.split('\\n') if line.strip()]\n",
        "        df = pd.DataFrame([{'Utterance': line} for line in lines])\n",
        "\n",
        "        perform_sentiment_analysis(df)\n",
        "    else:\n",
        "        print(\"No text could be extracted from the file.\")\n",
        "\n",
        "def perform_sentiment_analysis(df):\n",
        "    sentiment_results = []\n",
        "    for index, row in df.iterrows():\n",
        "        sentiment = finbert_sentiment(row['Utterance'])\n",
        "        sentiment_results.append((\n",
        "            sentiment['positive'],\n",
        "            sentiment['negative'],\n",
        "            sentiment['neutral'],\n",
        "            sentiment['sentiment']\n",
        "        ))\n",
        "\n",
        "    sentiment_df = pd.DataFrame(sentiment_results, columns=['finbert_pos', 'finbert_neg', 'finbert_neu', 'finbert_sentiment'])\n",
        "    df = pd.concat([df, sentiment_df], axis=1)\n",
        "\n",
        "    print(\"\\nDataFrame after Sentiment Analysis:\")\n",
        "    print(df)\n",
        "\n",
        "    print(\"\\nDetailed Analysis:\")\n",
        "    for index, row in df.iterrows():\n",
        "        print(f\"Utterance: {row['Utterance']}, Sentiment: {row['finbert_sentiment']}, Scores: (Pos: {row['finbert_pos']:.3f}, Neg: {row['finbert_neg']:.3f}, Neu: {row['finbert_neu']:.3f})\")\n",
        "\n",
        "    summary = df.agg({\n",
        "        'finbert_pos': 'sum',\n",
        "        'finbert_neg': 'sum',\n",
        "        'finbert_neu': 'sum'\n",
        "    })\n",
        "    summary['sentiment_score'] = summary['finbert_pos'] - summary['finbert_neg']\n",
        "\n",
        "    # Normalize score on a scale of 1 to 5\n",
        "    max_score = 5\n",
        "    sentiment_score_abs = abs(summary['sentiment_score'])  # Fix: Use abs() on the float\n",
        "    if sentiment_score_abs != 0:\n",
        "        normalized_overall_score = (summary['sentiment_score'] / sentiment_score_abs) * max_score\n",
        "    else:\n",
        "        normalized_overall_score = 1  # Handle zero-sentiment case\n",
        "\n",
        "    normalized_overall_score = max(1, min(normalized_overall_score, max_score))  # Clip to 1-5 range\n",
        "\n",
        "    overall_sentiment = 'positive' if summary['sentiment_score'] > 0 else 'negative' if summary['sentiment_score'] < 0 else 'neutral'\n",
        "\n",
        "    print(\"\\nOverall Sentiment Report:\")\n",
        "    print(f\"Overall Normalized Score (Scale 1-5): {normalized_overall_score:.2f}\")\n",
        "    print(f\"Overall Sentiment: {overall_sentiment}\")\n",
        "\n",
        "# Example usage\n",
        "file_path = '/content/Document6.pdf'  # Change this to the path for your DOCX or PDF\n",
        "process_conversation(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xi4zF_McLPoT",
        "outputId": "832fbf3a-4084-4a7a-8961-7bd0c2aa635b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (1.1.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "\n",
            "DataFrame after Sentiment Analysis:\n",
            "                                            Utterance  finbert_pos  \\\n",
            "0   Alex:  \"Maya, I don't understand why you're al...     0.999506   \n",
            "1   clothes and accessories. Don't you think it's ...     0.967153   \n",
            "2   Maya:  \"What's wrong with you, Alex? Can't I j...     0.708035   \n",
            "3                     always so uptight about money.\"     0.505753   \n",
            "4   Alex:  \"Uptight about money? You're the one wh...     0.993176   \n",
            "5   and a future to plan for. Your spending habits...     0.999770   \n",
            "6   Maya:  \"You're always so negative, Alex. Can't...     0.751160   \n",
            "7              decisions? I'm not a child, you know.\"     0.987912   \n",
            "8   Alex:  \"Trust you? You've consistently shown t...     0.624561   \n",
            "9                I'm worried about our future, Maya.\"     0.914995   \n",
            "10  Maya:  \"Well, maybe if you weren't so controll...     0.998839   \n",
            "11  your concerns. But the way you're talking to m...     0.999976   \n",
            "12  This conversation is negative because both Ale...     0.999998   \n",
            "13  and the conversation is not productive or resp...     0.895259   \n",
            "14  To turn this conversation positive, Alex and M...     0.998259   \n",
            "15  focusing on their shared goals and values, and...     0.999958   \n",
            "16                feelings and concerns. For example:     0.999981   \n",
            "17  Alex:  \"Maya, I feel worried when I see our ex...     0.940261   \n",
            "18  we're saving enough for our future. Can we tal...     0.915547   \n",
            "19                          prioritize our spending?\"     0.999216   \n",
            "20  Maya:  \"I understand where you're coming from,...     0.998594   \n",
            "21  every once in a while, but I also want to make...     0.999078   \n",
            "22  together to find a balance that works for both...     0.919452   \n",
            "23  By reframing the conversation in this way, Ale...     0.962707   \n",
            "24  respectful conversation about their financial ...     0.999917   \n",
            "\n",
            "    finbert_neg  finbert_neu finbert_sentiment  \n",
            "0             0            0          positive  \n",
            "1             0            0          positive  \n",
            "2             0            0          positive  \n",
            "3             0            0          positive  \n",
            "4             0            0          positive  \n",
            "5             0            0          positive  \n",
            "6             0            0          positive  \n",
            "7             0            0          positive  \n",
            "8             0            0          positive  \n",
            "9             0            0          positive  \n",
            "10            0            0          positive  \n",
            "11            0            0          positive  \n",
            "12            0            0          positive  \n",
            "13            0            0          positive  \n",
            "14            0            0          positive  \n",
            "15            0            0          positive  \n",
            "16            0            0          positive  \n",
            "17            0            0          positive  \n",
            "18            0            0          positive  \n",
            "19            0            0          positive  \n",
            "20            0            0          positive  \n",
            "21            0            0          positive  \n",
            "22            0            0          positive  \n",
            "23            0            0          positive  \n",
            "24            0            0          positive  \n",
            "\n",
            "Detailed Analysis:\n",
            "Utterance: Alex:  \"Maya, I don't understand why you're always spending so much money on designer, Sentiment: positive, Scores: (Pos: 1.000, Neg: 0.000, Neu: 0.000)\n",
            "Utterance: clothes and accessories. Don't you think it's a waste of our hard -earned cash?\", Sentiment: positive, Scores: (Pos: 0.967, Neg: 0.000, Neu: 0.000)\n",
            "Utterance: Maya:  \"What's wrong with you, Alex? Can't I just treat myself every once in a while? You're, Sentiment: positive, Scores: (Pos: 0.708, Neg: 0.000, Neu: 0.000)\n",
            "Utterance: always so uptight about money.\", Sentiment: positive, Scores: (Pos: 0.506, Neg: 0.000, Neu: 0.000)\n",
            "Utterance: Alex:  \"Uptight about money? You're the one who's being irresponsible. We have bills to pay, Sentiment: positive, Scores: (Pos: 0.993, Neg: 0.000, Neu: 0.000)\n",
            "Utterance: and a future to plan for. Your spending habits are going to put us in debt.\", Sentiment: positive, Scores: (Pos: 1.000, Neg: 0.000, Neu: 0.000)\n",
            "Utterance: Maya:  \"You're always so negative, Alex. Can't you just trust me to make good financial, Sentiment: positive, Scores: (Pos: 0.751, Neg: 0.000, Neu: 0.000)\n",
            "Utterance: decisions? I'm not a child, you know.\", Sentiment: positive, Scores: (Pos: 0.988, Neg: 0.000, Neu: 0.000)\n",
            "Utterance: Alex:  \"Trust you? You've consistently shown that you can't be trusted with our finances., Sentiment: positive, Scores: (Pos: 0.625, Neg: 0.000, Neu: 0.000)\n",
            "Utterance: I'm worried about our future, Maya.\", Sentiment: positive, Scores: (Pos: 0.915, Neg: 0.000, Neu: 0.000)\n",
            "Utterance: Maya:  \"Well, maybe if you weren't so controlling and critical, I'd be more willing to listen to, Sentiment: positive, Scores: (Pos: 0.999, Neg: 0.000, Neu: 0.000)\n",
            "Utterance: your concerns. But the way you're talking to me right now is really hurtful.\", Sentiment: positive, Scores: (Pos: 1.000, Neg: 0.000, Neu: 0.000)\n",
            "Utterance: This conversation is negative because both Alex and Maya are being critical and defensive,, Sentiment: positive, Scores: (Pos: 1.000, Neg: 0.000, Neu: 0.000)\n",
            "Utterance: and the conversation is not productive or respectful. [ 6], Sentiment: positive, Scores: (Pos: 0.895, Neg: 0.000, Neu: 0.000)\n",
            "Utterance: To turn this conversation positive, Alex and Maya could try to reframe the conversation by, Sentiment: positive, Scores: (Pos: 0.998, Neg: 0.000, Neu: 0.000)\n",
            "Utterance: focusing on their shared goals and values, and by using \"I\" statements to express their, Sentiment: positive, Scores: (Pos: 1.000, Neg: 0.000, Neu: 0.000)\n",
            "Utterance: feelings and concerns. For example:, Sentiment: positive, Scores: (Pos: 1.000, Neg: 0.000, Neu: 0.000)\n",
            "Utterance: Alex:  \"Maya, I feel worried when I see our expenses going up because I want to make sure, Sentiment: positive, Scores: (Pos: 0.940, Neg: 0.000, Neu: 0.000)\n",
            "Utterance: we're saving enough for our future. Can we talk about how we can work together to, Sentiment: positive, Scores: (Pos: 0.916, Neg: 0.000, Neu: 0.000)\n",
            "Utterance: prioritize our spending?\", Sentiment: positive, Scores: (Pos: 0.999, Neg: 0.000, Neu: 0.000)\n",
            "Utterance: Maya:  \"I understand where you're coming from, Alex. I feel like I deserve to treat myself, Sentiment: positive, Scores: (Pos: 0.999, Neg: 0.000, Neu: 0.000)\n",
            "Utterance: every once in a while, but I also want to make sure we're financially secure. Let's work, Sentiment: positive, Scores: (Pos: 0.999, Neg: 0.000, Neu: 0.000)\n",
            "Utterance: together to find a balance that works for both of us.\", Sentiment: positive, Scores: (Pos: 0.919, Neg: 0.000, Neu: 0.000)\n",
            "Utterance: By reframing the conversation in this way, Alex and Maya can have a more productive and, Sentiment: positive, Scores: (Pos: 0.963, Neg: 0.000, Neu: 0.000)\n",
            "Utterance: respectful conversation about their financial goals and priorities., Sentiment: positive, Scores: (Pos: 1.000, Neg: 0.000, Neu: 0.000)\n",
            "\n",
            "Overall Sentiment Report:\n",
            "Overall Normalized Score (Scale 1-5): 5.00\n",
            "Overall Sentiment: positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required libraries (run this in your Colab notebook)\n",
        "#!pip install pandas PyPDF2 python-docx transformers torch\n",
        "\n",
        "import pandas as pd\n",
        "import PyPDF2\n",
        "import docx\n",
        "import os\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load Hugging Face sentiment analysis pipeline with FinBERT\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"yiyanghkust/finbert-tone\")\n",
        "\n",
        "def finbert_sentiment(text):\n",
        "    result = sentiment_pipeline(text)[0]\n",
        "    return result['label']  # Return only the sentiment label\n",
        "\n",
        "def extract_from_pdf(file_path):\n",
        "    pdf_text = \"\"\n",
        "    with open(file_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                pdf_text += page_text + \"\\n\"\n",
        "    return pdf_text.strip()\n",
        "\n",
        "def extract_from_docx(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    doc_text = '\\n'.join([paragraph.text for paragraph in doc.paragraphs if paragraph.text.strip()])\n",
        "    return doc_text.strip()\n",
        "\n",
        "def process_conversation(file_path):\n",
        "    file_extension = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    if file_extension == '.pdf':\n",
        "        conversation_text = extract_from_pdf(file_path)\n",
        "    elif file_extension == '.docx':\n",
        "        conversation_text = extract_from_docx(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please provide a PDF or DOCX file.\")\n",
        "\n",
        "    if conversation_text:\n",
        "        # Split the conversation by lines\n",
        "        lines = [line.strip() for line in conversation_text.split('\\n') if line.strip()]\n",
        "        df = pd.DataFrame([{'Utterance': line} for line in lines])\n",
        "\n",
        "        perform_sentiment_analysis(df)\n",
        "    else:\n",
        "        print(\"No text could be extracted from the file.\")\n",
        "\n",
        "def perform_sentiment_analysis(df):\n",
        "    sentiment_results = []\n",
        "    for index, row in df.iterrows():\n",
        "        sentiment = finbert_sentiment(row['Utterance'])\n",
        "        sentiment_results.append(sentiment)\n",
        "\n",
        "    df['sentiment'] = sentiment_results  # Add sentiment results to the DataFrame\n",
        "\n",
        "    print(\"\\nDataFrame after Sentiment Analysis:\")\n",
        "    print(df)\n",
        "\n",
        "    # Overall sentiment\n",
        "    overall_sentiment = df['sentiment'].value_counts().idxmax()  # Most frequent sentiment\n",
        "    print(\"\\nOverall Sentiment of the Conversation:\")\n",
        "    print(overall_sentiment)\n",
        "\n",
        "# Example usage\n",
        "file_path = '/content/Document6.pdf'  # Change this to the path for your DOCX or PDF\n",
        "process_conversation(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiZcsykoMp-V",
        "outputId": "eb6fd38f-5ad1-4bb9-86fe-63ed2caceda6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame after Sentiment Analysis:\n",
            "                                            Utterance sentiment\n",
            "0   Alex:  \"Maya, I don't understand why you're al...   Neutral\n",
            "1   clothes and accessories. Don't you think it's ...   Neutral\n",
            "2   Maya:  \"What's wrong with you, Alex? Can't I j...  Negative\n",
            "3                     always so uptight about money.\"  Positive\n",
            "4   Alex:  \"Uptight about money? You're the one wh...   Neutral\n",
            "5   and a future to plan for. Your spending habits...   Neutral\n",
            "6   Maya:  \"You're always so negative, Alex. Can't...  Positive\n",
            "7              decisions? I'm not a child, you know.\"   Neutral\n",
            "8   Alex:  \"Trust you? You've consistently shown t...   Neutral\n",
            "9                I'm worried about our future, Maya.\"  Negative\n",
            "10  Maya:  \"Well, maybe if you weren't so controll...   Neutral\n",
            "11  your concerns. But the way you're talking to m...  Negative\n",
            "12  This conversation is negative because both Ale...  Negative\n",
            "13  and the conversation is not productive or resp...  Negative\n",
            "14  To turn this conversation positive, Alex and M...   Neutral\n",
            "15  focusing on their shared goals and values, and...   Neutral\n",
            "16                feelings and concerns. For example:  Negative\n",
            "17  Alex:  \"Maya, I feel worried when I see our ex...  Negative\n",
            "18  we're saving enough for our future. Can we tal...   Neutral\n",
            "19                          prioritize our spending?\"   Neutral\n",
            "20  Maya:  \"I understand where you're coming from,...   Neutral\n",
            "21  every once in a while, but I also want to make...  Positive\n",
            "22  together to find a balance that works for both...   Neutral\n",
            "23  By reframing the conversation in this way, Ale...  Positive\n",
            "24  respectful conversation about their financial ...   Neutral\n",
            "\n",
            "Overall Sentiment of the Conversation:\n",
            "Neutral\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oYFVIa1AN6OM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required libraries (run this in your Colab notebook)\n",
        "#!pip install pandas PyPDF2 python-docx transformers torch\n",
        "\n",
        "import pandas as pd\n",
        "import PyPDF2\n",
        "import docx\n",
        "import os\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load Hugging Face sentiment analysis pipeline with FinBERT\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"yiyanghkust/finbert-tone\")\n",
        "\n",
        "def finbert_sentiment(text):\n",
        "    result = sentiment_pipeline(text)[0]\n",
        "    sentiment_label = result['label']\n",
        "    score = result['score']\n",
        "\n",
        "    if sentiment_label == 'negative':\n",
        "        return {'positive': 0, 'negative': score, 'neutral': 0, 'sentiment': 'negative'}\n",
        "    elif sentiment_label == 'neutral':\n",
        "        return {'positive': 0, 'negative': 0, 'neutral': score, 'sentiment': 'neutral'}\n",
        "    else:  # Positive\n",
        "        return {'positive': score, 'negative': 0, 'neutral': 0, 'sentiment': 'positive'}\n",
        "\n",
        "# PDF extraction function\n",
        "def extract_from_pdf(file_path):\n",
        "    pdf_text = \"\"\n",
        "    with open(file_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:  # Ensure non-empty text is extracted\n",
        "                pdf_text += page_text + \"\\n\"\n",
        "    return pdf_text.strip()\n",
        "\n",
        "# DOCX extraction function\n",
        "def extract_from_docx(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    return '\\n'.join([paragraph.text for paragraph in doc.paragraphs if paragraph.text.strip()]).strip()\n",
        "\n",
        "def process_conversation(file_path):\n",
        "    file_extension = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    if file_extension == '.pdf':\n",
        "        conversation_text = extract_from_pdf(file_path)\n",
        "    elif file_extension == '.docx':\n",
        "        conversation_text = extract_from_docx(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please provide a PDF or DOCX file.\")\n",
        "\n",
        "    if conversation_text:\n",
        "        lines = [line.strip() for line in conversation_text.split('\\n') if line.strip()]\n",
        "        df = pd.DataFrame([{'Utterance': line} for line in lines])\n",
        "        perform_sentiment_analysis(df)\n",
        "    else:\n",
        "        print(\"No text could be extracted from the file.\")\n",
        "\n",
        "def perform_sentiment_analysis(df):\n",
        "    sentiment_results = []\n",
        "    for index, row in df.iterrows():\n",
        "        sentiment = finbert_sentiment(row['Utterance'])\n",
        "        sentiment_results.append((sentiment['positive'], sentiment['negative'], sentiment['neutral'], sentiment['sentiment']))\n",
        "\n",
        "    sentiment_df = pd.DataFrame(sentiment_results, columns=['finbert_pos', 'finbert_neg', 'finbert_neu', 'finbert_sentiment'])\n",
        "    df = pd.concat([df, sentiment_df], axis=1)\n",
        "\n",
        "    summary = df.agg({\n",
        "        'finbert_pos': 'sum',\n",
        "        'finbert_neg': 'sum',\n",
        "        'finbert_neu': 'sum'\n",
        "    })\n",
        "    summary['sentiment_score'] = summary['finbert_pos'] - summary['finbert_neg']\n",
        "\n",
        "    # Normalize score on a scale of 1 to 5\n",
        "    max_score = 5\n",
        "    sentiment_score_abs = abs(summary['sentiment_score'])\n",
        "    normalized_overall_score = (summary['sentiment_score'] / sentiment_score_abs * max_score) if sentiment_score_abs != 0 else 1\n",
        "    normalized_overall_score = max(1, min(normalized_overall_score, max_score))  # Clip to 1-5 range\n",
        "\n",
        "    overall_sentiment = 'positive' if summary['sentiment_score'] > 0 else 'negative' if summary['sentiment_score'] < 0 else 'neutral'\n",
        "\n",
        "    print(f\"Overall Normalized Score (Scale 1-5): {normalized_overall_score:.2f}\")\n",
        "    print(f\"Overall Sentiment: {overall_sentiment}\")\n",
        "\n",
        "# Example usage\n",
        "file_path = '/content/Document6.pdf'  # Change this to the path for your DOCX or PDF\n",
        "process_conversation(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaTlgYWJNcjt",
        "outputId": "1b64f876-11d2-4422-fef7-fcd72bd90fcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Normalized Score (Scale 1-5): 5.00\n",
            "Overall Sentiment: positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import PyPDF2\n",
        "import docx\n",
        "import os\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load FinBERT model and tokenizer\n",
        "model_name = \"yiyanghkust/finbert-tone\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "def finbert_sentiment(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    probabilities = torch.nn.functional.softmax(logits, dim=-1).numpy()[0]\n",
        "    sentiment = ['negative', 'neutral', 'positive'][probabilities.argmax()]\n",
        "\n",
        "    return {\n",
        "        'positive': probabilities[2],\n",
        "        'negative': probabilities[0],\n",
        "        'neutral': probabilities[1],\n",
        "        'sentiment': sentiment\n",
        "    }\n",
        "\n",
        "def extract_conversation_from_file(file_path):\n",
        "    file_extension = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    if file_extension == '.csv':\n",
        "        return extract_from_csv(file_path)\n",
        "    elif file_extension == '.pdf':\n",
        "        return extract_from_pdf(file_path)\n",
        "    elif file_extension == '.docx':\n",
        "        return extract_from_docx(file_path)\n",
        "    elif file_extension == '.xlsx':\n",
        "        return extract_from_excel(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please provide CSV, PDF, DOCX, or XLSX file.\")\n",
        "\n",
        "def extract_from_csv(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    if 'Speaker' in df.columns and 'Utterance' in df.columns:\n",
        "        return df[['Speaker', 'Utterance']]\n",
        "    else:\n",
        "        raise ValueError(\"CSV must contain 'Speaker' and 'Utterance' columns.\")\n",
        "\n",
        "def extract_from_pdf(file_path):\n",
        "    pdf_text = \"\"\n",
        "    with open(file_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            pdf_text += page.extract_text() + \"\\n\"\n",
        "    return pdf_text.strip()\n",
        "\n",
        "def extract_from_docx(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    doc_text = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
        "    return doc_text.strip()\n",
        "\n",
        "def extract_from_excel(file_path):\n",
        "    df = pd.read_excel(file_path)\n",
        "    if 'Speaker' in df.columns and 'Utterance' in df.columns:\n",
        "        return df[['Speaker', 'Utterance']]\n",
        "    else:\n",
        "        raise ValueError(\"Excel file must contain 'Speaker' and 'Utterance' columns.\")\n",
        "\n",
        "def process_pdf_conversation(pdf_text):\n",
        "    lines = [line.strip() for line in pdf_text.split('\\n') if line.strip()]\n",
        "    speakers = ['Interviewer', 'Financial Expert']\n",
        "    dialogue = []\n",
        "\n",
        "    current_speaker = None\n",
        "    for line in lines:\n",
        "        for speaker in speakers:\n",
        "            if line.startswith(speaker + \":\"):\n",
        "                current_speaker = speaker\n",
        "                utterance = line[len(speaker) + 1:].strip()\n",
        "                dialogue.append((current_speaker, utterance))\n",
        "                break\n",
        "\n",
        "    df = pd.DataFrame(dialogue, columns=['Speaker', 'Utterance'])\n",
        "    return df\n",
        "\n",
        "def process_conversation(file_path):\n",
        "    try:\n",
        "        extracted_data = extract_conversation_from_file(file_path)\n",
        "\n",
        "        if isinstance(extracted_data, pd.DataFrame):\n",
        "            print(\"Initial DataFrame:\")\n",
        "            print(extracted_data.head())\n",
        "            perform_sentiment_analysis(extracted_data)\n",
        "        else:\n",
        "            df = process_pdf_conversation(extracted_data)\n",
        "            print(\"\\nExtracted Dialogue DataFrame:\")\n",
        "            print(df)\n",
        "\n",
        "            perform_sentiment_analysis(df)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing the file: {e}\")\n",
        "\n",
        "def perform_sentiment_analysis(df):\n",
        "    sentiment_results = []\n",
        "    for index, row in df.iterrows():\n",
        "        sentiment = finbert_sentiment(row['Utterance'])\n",
        "        sentiment_results.append((\n",
        "            sentiment['positive'],\n",
        "            sentiment['negative'],\n",
        "            sentiment['neutral'],\n",
        "            sentiment['sentiment']\n",
        "        ))\n",
        "\n",
        "    sentiment_df = pd.DataFrame(sentiment_results, columns=['finbert_pos', 'finbert_neg', 'finbert_neu', 'finbert_sentiment'])\n",
        "    df = pd.concat([df, sentiment_df], axis=1)\n",
        "\n",
        "    print(\"\\nDataFrame after Sentiment Analysis:\")\n",
        "    print(df)\n",
        "\n",
        "    print(\"\\nDetailed Analysis:\")\n",
        "    for index, row in df.iterrows():\n",
        "        full_utterance = row['Utterance']\n",
        "        print(f\"Speaker: {row['Speaker']}, Utterance: {full_utterance}, Sentiment: {row['finbert_sentiment']}, Scores: (Pos: {row['finbert_pos']:.3f}, Neg: {row['finbert_neg']:.3f}, Neu: {row['finbert_neu']:.3f})\")\n",
        "\n",
        "    summary = df.groupby('Speaker').agg({\n",
        "        'finbert_pos': 'sum',\n",
        "        'finbert_neg': 'sum',\n",
        "        'finbert_neu': 'sum'\n",
        "    })\n",
        "    summary['sentiment_score'] = summary['finbert_pos'] - summary['finbert_neg']\n",
        "    print(\"\\nSummary by Speaker:\")\n",
        "    print(summary)\n",
        "\n",
        "    overall_score = summary['sentiment_score'].sum()\n",
        "    overall_sentiment = 'positive' if overall_score > 0 else 'negative'\n",
        "    print(\"\\nOverall Sentiment Report:\")\n",
        "    print(f\"Overall Score: {overall_score:.2f}\")\n",
        "    print(f\"Overall Sentiment: {overall_sentiment}\")\n",
        "\n",
        "# Example usage\n",
        "file_path = '/content/Document6.pdf'  # Change to your file path for CSV, PDF, DOCX, or XLSX\n",
        "process_conversation(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdg8Md1cONRK",
        "outputId": "1268b62e-8040-4093-8f48-b892c6099b20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracted Dialogue DataFrame:\n",
            "Empty DataFrame\n",
            "Columns: [Speaker, Utterance]\n",
            "Index: []\n",
            "\n",
            "DataFrame after Sentiment Analysis:\n",
            "Empty DataFrame\n",
            "Columns: [Speaker, Utterance, finbert_pos, finbert_neg, finbert_neu, finbert_sentiment]\n",
            "Index: []\n",
            "\n",
            "Detailed Analysis:\n",
            "\n",
            "Summary by Speaker:\n",
            "Empty DataFrame\n",
            "Columns: [finbert_pos, finbert_neg, finbert_neu, sentiment_score]\n",
            "Index: []\n",
            "\n",
            "Overall Sentiment Report:\n",
            "Overall Score: 0.00\n",
            "Overall Sentiment: negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import PyPDF2\n",
        "import docx\n",
        "import os\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load FinBERT model and tokenizer\n",
        "model_name = \"yiyanghkust/finbert-tone\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "def finbert_sentiment(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    probabilities = torch.nn.functional.softmax(logits, dim=-1).numpy()[0]\n",
        "    sentiment = ['negative', 'neutral', 'positive'][probabilities.argmax()]\n",
        "\n",
        "    return {\n",
        "        'positive': probabilities[2],\n",
        "        'negative': probabilities[0],\n",
        "        'neutral': probabilities[1],\n",
        "        'sentiment': sentiment\n",
        "    }\n",
        "\n",
        "def extract_conversation_from_file(file_path):\n",
        "    file_extension = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    if file_extension == '.csv':\n",
        "        return extract_from_csv(file_path)\n",
        "    elif file_extension == '.pdf':\n",
        "        return extract_from_pdf(file_path)\n",
        "    elif file_extension == '.docx':\n",
        "        return extract_from_docx(file_path)\n",
        "    elif file_extension == '.xlsx':\n",
        "        return extract_from_excel(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please provide CSV, PDF, DOCX, or XLSX file.\")\n",
        "\n",
        "def extract_from_csv(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    if 'Speaker' in df.columns and 'Utterance' in df.columns:\n",
        "        return df[['Speaker', 'Utterance']]\n",
        "    else:\n",
        "        raise ValueError(\"CSV must contain 'Speaker' and 'Utterance' columns.\")\n",
        "\n",
        "def extract_from_pdf(file_path):\n",
        "    pdf_text = \"\"\n",
        "    with open(file_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            pdf_text += page.extract_text() + \"\\n\"\n",
        "    return pdf_text.strip()\n",
        "\n",
        "def extract_from_docx(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    doc_text = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
        "    return doc_text.strip()\n",
        "\n",
        "def extract_from_excel(file_path):\n",
        "    df = pd.read_excel(file_path)\n",
        "    if 'Speaker' in df.columns and 'Utterance' in df.columns:\n",
        "        return df[['Speaker', 'Utterance']]\n",
        "    else:\n",
        "        raise ValueError(\"Excel file must contain 'Speaker' and 'Utterance' columns.\")\n",
        "\n",
        "def process_pdf_conversation(pdf_text):\n",
        "    lines = [line.strip() for line in pdf_text.split('\\n') if line.strip()]\n",
        "    speakers = ['Interviewer', 'Financial Expert']\n",
        "    dialogue = []\n",
        "\n",
        "    current_speaker = None\n",
        "    for line in lines:\n",
        "        for speaker in speakers:\n",
        "            if line.startswith(speaker + \":\"):\n",
        "                current_speaker = speaker\n",
        "                utterance = line[len(speaker) + 1:].strip()\n",
        "                dialogue.append((current_speaker, utterance))\n",
        "                break\n",
        "\n",
        "    df = pd.DataFrame(dialogue, columns=['Speaker', 'Utterance'])\n",
        "    return df\n",
        "\n",
        "def process_conversation(file_path):\n",
        "    try:\n",
        "        extracted_data = extract_conversation_from_file(file_path)\n",
        "\n",
        "        if isinstance(extracted_data, pd.DataFrame):\n",
        "            if extracted_data.empty:\n",
        "                print(\"The provided file resulted in an empty DataFrame. No data to process.\")\n",
        "                return\n",
        "\n",
        "            print(\"Initial DataFrame:\")\n",
        "            print(extracted_data.head())\n",
        "\n",
        "            # Drop rows with missing 'Utterance'\n",
        "            extracted_data = extracted_data.dropna(subset=['Utterance'])\n",
        "\n",
        "            if extracted_data.empty:\n",
        "                print(\"All rows have missing 'Utterance'. Nothing to analyze.\")\n",
        "                return\n",
        "\n",
        "            perform_sentiment_analysis(extracted_data)\n",
        "        else:\n",
        "            df = process_pdf_conversation(extracted_data)\n",
        "            if df.empty:\n",
        "                print(\"The extracted dialogue resulted in an empty DataFrame. No data to process.\")\n",
        "                return\n",
        "\n",
        "            print(\"\\nExtracted Dialogue DataFrame:\")\n",
        "            print(df.head())\n",
        "\n",
        "            perform_sentiment_analysis(df)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing the file: {e}\")\n",
        "\n",
        "def perform_sentiment_analysis(df):\n",
        "    if df.empty:\n",
        "        print(\"No data available for sentiment analysis.\")\n",
        "        return\n",
        "\n",
        "    sentiment_results = []\n",
        "    for index, row in df.iterrows():\n",
        "        utterance = row.get('Utterance', '')\n",
        "        if not utterance.strip():\n",
        "            print(f\"Skipping empty or invalid utterance at index {index}.\")\n",
        "            continue\n",
        "\n",
        "        sentiment = finbert_sentiment(utterance)\n",
        "        sentiment_results.append((\n",
        "            sentiment['positive'],\n",
        "            sentiment['negative'],\n",
        "            sentiment['neutral'],\n",
        "            sentiment['sentiment']\n",
        "        ))\n",
        "\n",
        "    if not sentiment_results:\n",
        "        print(\"No valid utterances for sentiment analysis.\")\n",
        "        return\n",
        "\n",
        "    sentiment_df = pd.DataFrame(sentiment_results, columns=['finbert_pos', 'finbert_neg', 'finbert_neu', 'finbert_sentiment'])\n",
        "    df = pd.concat([df.reset_index(drop=True), sentiment_df], axis=1)  # Resetting index to avoid conflicts during concat\n",
        "\n",
        "    print(\"\\nDataFrame after Sentiment Analysis:\")\n",
        "    print(df)\n",
        "\n",
        "    # Additional analysis and summaries\n",
        "    if not df.empty:\n",
        "        detailed_analysis(df)\n",
        "\n",
        "def detailed_analysis(df):\n",
        "    print(\"\\nDetailed Analysis:\")\n",
        "\n",
        "    # Confidence threshold (e.g., only consider sentiments with a probability > 0.6)\n",
        "    confidence_threshold = 0.6\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        full_utterance = row['Utterance']\n",
        "        pos_score = row['finbert_pos']\n",
        "        neg_score = row['finbert_neg']\n",
        "        neu_score = row['finbert_neu']\n",
        "\n",
        "        if max(pos_score, neg_score, neu_score) < confidence_threshold:\n",
        "            print(f\"Skipping low-confidence sentiment analysis for utterance: {full_utterance}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Speaker: {row['Speaker']}, Utterance: {full_utterance}, Sentiment: {row['finbert_sentiment']}, Scores: (Pos: {pos_score:.3f}, Neg: {neg_score:.3f}, Neu: {neu_score:.3f})\")\n",
        "\n",
        "    # Filter out low-confidence rows\n",
        "    confident_df = df[df[['finbert_pos', 'finbert_neg', 'finbert_neu']].max(axis=1) >= confidence_threshold].copy()\n",
        "\n",
        "    # Custom weighted sentiment score\n",
        "    confident_df.loc[:, 'weighted_sentiment_score'] = confident_df['finbert_pos'] * 2 - confident_df['finbert_neg'] * 2\n",
        "\n",
        "    # Group by speaker\n",
        "    summary = confident_df.groupby('Speaker').agg({\n",
        "        'finbert_pos': 'sum',\n",
        "        'finbert_neg': 'sum',\n",
        "        'finbert_neu': 'sum',\n",
        "        'weighted_sentiment_score': 'sum'\n",
        "    })\n",
        "    print(\"\\nSummary by Speaker:\")\n",
        "    print(summary)\n",
        "\n",
        "    # Calculate overall score\n",
        "    overall_score = summary['weighted_sentiment_score'].sum()\n",
        "    overall_sentiment = 'positive' if overall_score > 0 else 'negative'\n",
        "    print(\"\\nOverall Sentiment Report:\")\n",
        "    print(f\"Overall Score: {overall_score:.2f}\")\n",
        "    print(f\"Overall Sentiment: {overall_sentiment}\")\n",
        "\n",
        "# Example usage\n",
        "file_path = '/content/Trancription.csv'  # Change to your file path for CSV, PDF, DOCX, or XLSX\n",
        "process_conversation(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaDH0lNBTKFC",
        "outputId": "ce475954-ee08-4bc1-c304-c275bf49fa06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial DataFrame:\n",
            "   Speaker                                          Utterance\n",
            "0  Sandeep   One thing which I've taken away for the wealt...\n",
            "1  Ranveer   People are running out of money because of ba...\n",
            "2  Sandeep   No, they're in fact, they will live so long. ...\n",
            "3  Ranveer   What do you think Adani and Mukesham money do...\n",
            "4  Ranveer   Now there's countless content pieces on YouTu...\n",
            "\n",
            "DataFrame after Sentiment Analysis:\n",
            "   Speaker                                          Utterance  finbert_pos  \\\n",
            "0  Sandeep   One thing which I've taken away for the wealt...     0.039660   \n",
            "1  Ranveer   People are running out of money because of ba...     0.521271   \n",
            "2  Sandeep   No, they're in fact, they will live so long. ...     0.001554   \n",
            "3  Ranveer   What do you think Adani and Mukesham money do...     0.000037   \n",
            "4  Ranveer   Now there's countless content pieces on YouTu...     0.000009   \n",
            "\n",
            "   finbert_neg   finbert_neu finbert_sentiment  \n",
            "0     0.026629  9.337113e-01           neutral  \n",
            "1     0.478692  3.688753e-05          positive  \n",
            "2     0.998378  6.781126e-05          negative  \n",
            "3     0.999963  5.250359e-07          negative  \n",
            "4     0.999927  6.371331e-05          negative  \n",
            "\n",
            "Detailed Analysis:\n",
            "Speaker: Sandeep, Utterance:  One thing which I've taken away for the wealthy is that they are very serious about growth. They make sure that the money is compounding over a long period of time. The US by the way, that people are running out of money in their older age., Sentiment: neutral, Scores: (Pos: 0.040, Neg: 0.027, Neu: 0.934)\n",
            "Skipping low-confidence sentiment analysis for utterance:  People are running out of money because of bad planning or because of inflation.\n",
            "Speaker: Sandeep, Utterance:  No, they're in fact, they will live so long. Oh shit., Sentiment: negative, Scores: (Pos: 0.002, Neg: 0.998, Neu: 0.000)\n",
            "Speaker: Ranveer, Utterance:  What do you think Adani and Mukesham money do with their own money? And by that I mean not reliance his money, Adani groups money, their own money as men., Sentiment: negative, Scores: (Pos: 0.000, Neg: 1.000, Neu: 0.000)\n",
            "Speaker: Ranveer, Utterance:  Now there's countless content pieces on YouTube on Spotify where you can learn about money, but it's these kind of conversations with experts from the world of finance people who manage money in the range of 50,000 rupees and savings to 50 crores and savings per month. That's the experience, the financial know how of our guest today. It's Sandeep Jaitwani on the show. His startup Dizerve.in is a modern day portfolio management service. Personally for me, most of my money is handled not by myself, but by portfolio management services. I trust this man enough to give him my own money. He is handling some of beer by himself media world's funds. That's why I thought that on this episode, let me give you guys access to someone who is given me financial advice, someone that I trust my money with. It's going to be a very raw and open conversation about everything from how the richest in India manages their money to how you can manage your money even if you're just beginning your career right now. It's a financial edu key., Sentiment: negative, Scores: (Pos: 0.000, Neg: 1.000, Neu: 0.000)\n",
            "\n",
            "Summary by Speaker:\n",
            "         finbert_pos  finbert_neg  finbert_neu  weighted_sentiment_score\n",
            "Speaker                                                                 \n",
            "Ranveer     0.000046     1.999890     0.000064                 -3.999687\n",
            "Sandeep     0.041214     1.025007     0.933779                 -1.967586\n",
            "\n",
            "Overall Sentiment Report:\n",
            "Overall Score: -5.97\n",
            "Overall Sentiment: negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sylkjVfwgO8",
        "outputId": "d5bba759-6b5c-4142-fe04-6a9c13e6c08a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import PyPDF2\n",
        "import docx\n",
        "import os\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load FinBERT model and tokenizer\n",
        "model_name = \"yiyanghkust/finbert-tone\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "def finbert_sentiment(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    probabilities = torch.nn.functional.softmax(logits, dim=-1).numpy()[0]\n",
        "    sentiment = ['negative', 'neutral', 'positive'][probabilities.argmax()]\n",
        "\n",
        "    return {\n",
        "        'positive': probabilities[2],\n",
        "        'negative': probabilities[0],\n",
        "        'neutral': probabilities[1],\n",
        "        'sentiment': sentiment\n",
        "    }\n",
        "\n",
        "def extract_conversation_from_file(file_path):\n",
        "    file_extension = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    if file_extension == '.csv':\n",
        "        return extract_from_csv(file_path)\n",
        "    elif file_extension == '.pdf':\n",
        "        return extract_from_pdf(file_path)\n",
        "    elif file_extension == '.docx':\n",
        "        return extract_from_docx(file_path)\n",
        "    elif file_extension == '.xlsx':\n",
        "        return extract_from_excel(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please provide CSV, PDF, DOCX, or XLSX file.\")\n",
        "\n",
        "def extract_from_csv(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    if 'Speaker' in df.columns and 'Utterance' in df.columns:\n",
        "        return df[['Speaker', 'Utterance']]\n",
        "    else:\n",
        "        raise ValueError(\"CSV must contain 'Speaker' and 'Utterance' columns.\")\n",
        "\n",
        "def extract_from_pdf(file_path):\n",
        "    pdf_text = \"\"\n",
        "    with open(file_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            pdf_text += page.extract_text() + \"\\n\"\n",
        "    return pdf_text.strip()\n",
        "\n",
        "def extract_from_docx(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    doc_text = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
        "    return doc_text.strip()\n",
        "\n",
        "def extract_from_excel(file_path):\n",
        "    df = pd.read_excel(file_path)\n",
        "    if 'Speaker' in df.columns and 'Utterance' in df.columns:\n",
        "        return df[['Speaker', 'Utterance']]\n",
        "    else:\n",
        "        raise ValueError(\"Excel file must contain 'Speaker' and 'Utterance' columns.\")\n",
        "\n",
        "def process_pdf_conversation(pdf_text):\n",
        "    lines = [line.strip() for line in pdf_text.split('\\n') if line.strip()]\n",
        "    speakers = ['Interviewer', 'Financial Expert']\n",
        "    dialogue = []\n",
        "\n",
        "    current_speaker = None\n",
        "    for line in lines:\n",
        "        for speaker in speakers:\n",
        "            if line.startswith(speaker + \":\"):\n",
        "                current_speaker = speaker\n",
        "                utterance = line[len(speaker) + 1:].strip()\n",
        "                dialogue.append((current_speaker, utterance))\n",
        "                break\n",
        "\n",
        "    df = pd.DataFrame(dialogue, columns=['Speaker', 'Utterance'])\n",
        "    return df\n",
        "\n",
        "def process_conversation(file_path):\n",
        "    try:\n",
        "        extracted_data = extract_conversation_from_file(file_path)\n",
        "\n",
        "        if isinstance(extracted_data, pd.DataFrame):\n",
        "            if extracted_data.empty:\n",
        "                print(\"The provided file resulted in an empty DataFrame. No data to process.\")\n",
        "                return\n",
        "\n",
        "            print(\"Initial DataFrame:\")\n",
        "            print(extracted_data.head())\n",
        "\n",
        "            # Drop rows with missing 'Utterance'\n",
        "            extracted_data = extracted_data.dropna(subset=['Utterance'])\n",
        "\n",
        "            if extracted_data.empty:\n",
        "                print(\"All rows have missing 'Utterance'. Nothing to analyze.\")\n",
        "                return\n",
        "\n",
        "            perform_sentiment_analysis(extracted_data)\n",
        "        else:\n",
        "            df = process_pdf_conversation(extracted_data)\n",
        "            if df.empty:\n",
        "                print(\"The extracted dialogue resulted in an empty DataFrame. No data to process.\")\n",
        "                return\n",
        "\n",
        "            print(\"\\nExtracted Dialogue DataFrame:\")\n",
        "            print(df.head())\n",
        "\n",
        "            perform_sentiment_analysis(df)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing the file: {e}\")\n",
        "\n",
        "def perform_sentiment_analysis(df):\n",
        "    if df.empty:\n",
        "        print(\"No data available for sentiment analysis.\")\n",
        "        return\n",
        "\n",
        "    sentiment_results = []\n",
        "    for index, row in df.iterrows():\n",
        "        utterance = row.get('Utterance', '')\n",
        "        if not utterance.strip():\n",
        "            print(f\"Skipping empty or invalid utterance at index {index}.\")\n",
        "            continue\n",
        "\n",
        "        sentiment = finbert_sentiment(utterance)\n",
        "        sentiment_results.append((\n",
        "            sentiment['positive'],\n",
        "            sentiment['negative'],\n",
        "            sentiment['neutral'],\n",
        "            sentiment['sentiment']\n",
        "        ))\n",
        "\n",
        "    if not sentiment_results:\n",
        "        print(\"No valid utterances for sentiment analysis.\")\n",
        "        return\n",
        "\n",
        "    sentiment_df = pd.DataFrame(sentiment_results, columns=['finbert_pos', 'finbert_neg', 'finbert_neu', 'finbert_sentiment'])\n",
        "    df = pd.concat([df, sentiment_df], axis=1)\n",
        "\n",
        "    print(\"\\nDataFrame after Sentiment Analysis:\")\n",
        "    print(df)\n",
        "\n",
        "    # Additional analysis and summaries\n",
        "    if not df.empty:\n",
        "        detailed_analysis(df)\n",
        "\n",
        "def detailed_analysis(df):\n",
        "    print(\"\\nDetailed Analysis:\")\n",
        "    for index, row in df.iterrows():\n",
        "        full_utterance = row['Utterance']\n",
        "        print(f\"Speaker: {row['Speaker']}, Utterance: {full_utterance}, Sentiment: {row['finbert_sentiment']}, Scores: (Pos: {row['finbert_pos']:.3f}, Neg: {row['finbert_neg']:.3f}, Neu: {row['finbert_neu']:.3f})\")\n",
        "\n",
        "    summary = df.groupby('Speaker').agg({\n",
        "        'finbert_pos': 'sum',\n",
        "        'finbert_neg': 'sum',\n",
        "        'finbert_neu': 'sum'\n",
        "    })\n",
        "    summary['sentiment_score'] = summary['finbert_pos'] - summary['finbert_neg']\n",
        "    print(\"\\nSummary by Speaker:\")\n",
        "    print(summary)\n",
        "\n",
        "    overall_score = summary['sentiment_score'].sum()\n",
        "    overall_sentiment = 'positive' if overall_score > 0 else 'negative'\n",
        "    print(\"\\nOverall Sentiment Report:\")\n",
        "    print(f\"Overall Score: {overall_score:.2f}\")\n",
        "    print(f\"Overall Sentiment: {overall_sentiment}\")\n",
        "\n",
        "# Example usage\n",
        "file_path = '/content/transcriptions (1).txt'  # Change to your file path for CSV, PDF, DOCX, or XLSX\n",
        "process_conversation(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCeW8_QuQxl2",
        "outputId": "36743139-8640-4539-8817-d2d39437612b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing the file: Unsupported file format. Please provide CSV, PDF, DOCX, or XLSX file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import PyPDF2\n",
        "import docx\n",
        "import os\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load FinBERT model and tokenizer\n",
        "model_name = \"yiyanghkust/finbert-tone\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "def finbert_sentiment(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    probabilities = torch.nn.functional.softmax(logits, dim=-1).numpy()[0]\n",
        "    sentiment = ['negative', 'neutral', 'positive'][probabilities.argmax()]\n",
        "\n",
        "    return {\n",
        "        'positive': probabilities[2],\n",
        "        'negative': probabilities[0],\n",
        "        'neutral': probabilities[1],\n",
        "        'sentiment': sentiment\n",
        "    }\n",
        "\n",
        "def extract_conversation_from_file(file_path):\n",
        "    file_extension = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    if file_extension == '.csv':\n",
        "        return extract_from_csv(file_path)\n",
        "    elif file_extension == '.pdf':\n",
        "        return extract_from_pdf(file_path)\n",
        "    elif file_extension == '.docx':\n",
        "        return extract_from_docx(file_path)\n",
        "    elif file_extension == '.xlsx':\n",
        "        return extract_from_excel(file_path)\n",
        "    elif file_extension == '.txt':\n",
        "        return extract_from_txt(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please provide CSV, PDF, DOCX, XLSX, or TXT file.\")\n",
        "\n",
        "def extract_from_csv(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    if 'Speaker' in df.columns and 'Utterance' in df.columns:\n",
        "        return df[['Speaker', 'Utterance']]\n",
        "    else:\n",
        "        raise ValueError(\"CSV must contain 'Speaker' and 'Utterance' columns.\")\n",
        "\n",
        "# Reverting to the previous PDF extraction\n",
        "def extract_from_pdf(file_path):\n",
        "    pdf_text = \"\"\n",
        "    with open(file_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        for page in reader.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                pdf_text += page_text + \"\\n\"\n",
        "    return pdf_text.strip()\n",
        "\n",
        "def extract_from_docx(file_path):\n",
        "    doc = docx.Document(file_path)\n",
        "    doc_text = '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
        "    return doc_text.strip()\n",
        "\n",
        "def extract_from_excel(file_path):\n",
        "    df = pd.read_excel(file_path)\n",
        "    if 'Speaker' in df.columns and 'Utterance' in df.columns:\n",
        "        return df[['Speaker', 'Utterance']]\n",
        "    else:\n",
        "        raise ValueError(\"Excel file must contain 'Speaker' and 'Utterance' columns.\")\n",
        "\n",
        "def extract_from_txt(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        text = file.read()\n",
        "    return text.strip()\n",
        "\n",
        "def process_pdf_conversation(pdf_text):\n",
        "    lines = [line.strip() for line in pdf_text.split('\\n') if line.strip()]\n",
        "    speakers = ['Interviewer', 'Financial Expert']  # Adjust based on your actual speaker names\n",
        "    dialogue = []\n",
        "\n",
        "    current_speaker = None\n",
        "    for line in lines:\n",
        "        for speaker in speakers:\n",
        "            if line.startswith(speaker + \":\"):\n",
        "                current_speaker = speaker\n",
        "                utterance = line[len(speaker) + 1:].strip()\n",
        "                dialogue.append((current_speaker, utterance))\n",
        "                break\n",
        "\n",
        "    df = pd.DataFrame(dialogue, columns=['Speaker', 'Utterance'])\n",
        "    return df\n",
        "\n",
        "def process_txt_conversation(txt_text):\n",
        "    lines = [line.strip() for line in txt_text.split('\\n') if line.strip()]\n",
        "    dialogue = []\n",
        "\n",
        "    for line in lines:\n",
        "        parts = line.split('\\t')  # Assuming the tab is the delimiter in the .txt file\n",
        "        if len(parts) >= 3:\n",
        "            speaker = parts[2]\n",
        "            utterance = parts[3]\n",
        "            dialogue.append((speaker, utterance))\n",
        "\n",
        "    df = pd.DataFrame(dialogue, columns=['Speaker', 'Utterance'])\n",
        "    return df\n",
        "\n",
        "def process_conversation(file_path):\n",
        "    try:\n",
        "        extracted_data = extract_conversation_from_file(file_path)\n",
        "\n",
        "        if isinstance(extracted_data, pd.DataFrame):\n",
        "            if extracted_data.empty:\n",
        "                print(\"The provided file resulted in an empty DataFrame. No data to process.\")\n",
        "                return\n",
        "\n",
        "            print(\"Initial DataFrame:\")\n",
        "            print(extracted_data.head())\n",
        "\n",
        "            # Drop rows with missing 'Utterance'\n",
        "            extracted_data = extracted_data.dropna(subset=['Utterance'])\n",
        "\n",
        "            if extracted_data.empty:\n",
        "                print(\"All rows have missing 'Utterance'. Nothing to analyze.\")\n",
        "                return\n",
        "\n",
        "            perform_sentiment_analysis(extracted_data)\n",
        "        else:\n",
        "            if file_path.endswith('.txt'):\n",
        "                df = process_txt_conversation(extracted_data)\n",
        "            else:\n",
        "                df = process_pdf_conversation(extracted_data)\n",
        "\n",
        "            if df.empty:\n",
        "                print(\"The extracted dialogue resulted in an empty DataFrame. No data to process.\")\n",
        "                return\n",
        "\n",
        "            print(\"\\nExtracted Dialogue DataFrame:\")\n",
        "            print(df.head())\n",
        "\n",
        "            perform_sentiment_analysis(df)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing the file: {e}\")\n",
        "\n",
        "def perform_sentiment_analysis(df):\n",
        "    if df.empty:\n",
        "        print(\"No data available for sentiment analysis.\")\n",
        "        return\n",
        "\n",
        "    sentiment_results = []\n",
        "    for index, row in df.iterrows():\n",
        "        utterance = row.get('Utterance', '')\n",
        "        if not utterance.strip():\n",
        "            print(f\"Skipping empty or invalid utterance at index {index}.\")\n",
        "            continue\n",
        "\n",
        "        sentiment = finbert_sentiment(utterance)\n",
        "        sentiment_results.append((\n",
        "            sentiment['positive'],\n",
        "            sentiment['negative'],\n",
        "            sentiment['neutral'],\n",
        "            sentiment['sentiment']\n",
        "        ))\n",
        "\n",
        "    if not sentiment_results:\n",
        "        print(\"No valid utterances for sentiment analysis.\")\n",
        "        return\n",
        "\n",
        "    sentiment_df = pd.DataFrame(sentiment_results, columns=['finbert_pos', 'finbert_neg', 'finbert_neu', 'finbert_sentiment'])\n",
        "    df = pd.concat([df, sentiment_df], axis=1)\n",
        "\n",
        "    print(\"\\nDataFrame after Sentiment Analysis:\")\n",
        "    print(df)\n",
        "\n",
        "    # Additional analysis and summaries\n",
        "    if not df.empty:\n",
        "        detailed_analysis(df)\n",
        "\n",
        "def detailed_analysis(df):\n",
        "    print(\"\\nDetailed Analysis:\")\n",
        "    for index, row in df.iterrows():\n",
        "        full_utterance = row['Utterance']\n",
        "        print(f\"Speaker: {row['Speaker']}, Utterance: {full_utterance}, Sentiment: {row['finbert_sentiment']}, Scores: (Pos: {row['finbert_pos']:.3f}, Neg: {row['finbert_neg']:.3f}, Neu: {row['finbert_neu']:.3f})\")\n",
        "\n",
        "    summary = df.groupby('Speaker').agg({\n",
        "        'finbert_pos': 'sum',\n",
        "        'finbert_neg': 'sum',\n",
        "        'finbert_neu': 'sum'\n",
        "    })\n",
        "    summary['sentiment_score'] = summary['finbert_pos'] - summary['finbert_neg']\n",
        "    print(\"\\nSummary by Speaker:\")\n",
        "    print(summary)\n",
        "\n",
        "    overall_score = summary['sentiment_score'].sum()\n",
        "    overall_sentiment = 'positive' if overall_score > 0 else 'negative'\n",
        "    print(\"\\nOverall Sentiment Report:\")\n",
        "    print(f\"Overall Score: {overall_score:.2f}\")\n",
        "    print(f\"Overall Sentiment: {overall_sentiment}\")\n",
        "\n",
        "# Example usage\n",
        "file_path = '/content/transcriptions (1).txt'  # Change to your file path\n",
        "process_conversation(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "as5lt5YCySVi",
        "outputId": "b8f56793-8f68-41d5-fdc7-2ff0a988b912"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracted Dialogue DataFrame:\n",
            "   Speaker                                          Utterance\n",
            "0  Speaker                                          Utterance\n",
            "1  Sandeep   One thing which I've taken away for the wealt...\n",
            "2  Ranveer   People are running out of money because of ba...\n",
            "3  Sandeep   No, they're in fact, they will live so long. ...\n",
            "4  Ranveer   What do you think Adani and Mukesham money do...\n",
            "\n",
            "DataFrame after Sentiment Analysis:\n",
            "   Speaker                                          Utterance  finbert_pos  \\\n",
            "0  Speaker                                          Utterance     0.000415   \n",
            "1  Sandeep   One thing which I've taken away for the wealt...     0.039660   \n",
            "2  Ranveer   People are running out of money because of ba...     0.521271   \n",
            "3  Sandeep   No, they're in fact, they will live so long. ...     0.001554   \n",
            "4  Ranveer   What do you think Adani and Mukesham money do...     0.000037   \n",
            "5  Ranveer   Now there's countless content pieces on YouTu...     0.000009   \n",
            "\n",
            "   finbert_neg   finbert_neu finbert_sentiment  \n",
            "0     0.999409  1.760173e-04          negative  \n",
            "1     0.026629  9.337113e-01           neutral  \n",
            "2     0.478692  3.688753e-05          positive  \n",
            "3     0.998378  6.781126e-05          negative  \n",
            "4     0.999963  5.250359e-07          negative  \n",
            "5     0.999927  6.371331e-05          negative  \n",
            "\n",
            "Detailed Analysis:\n",
            "Speaker: Speaker, Utterance: Utterance, Sentiment: negative, Scores: (Pos: 0.000, Neg: 0.999, Neu: 0.000)\n",
            "Speaker: Sandeep, Utterance:  One thing which I've taken away for the wealthy is that they are very serious about growth. They make sure that the money is compounding over a long period of time. The US by the way, that people are running out of money in their older age., Sentiment: neutral, Scores: (Pos: 0.040, Neg: 0.027, Neu: 0.934)\n",
            "Speaker: Ranveer, Utterance:  People are running out of money because of bad planning or because of inflation., Sentiment: positive, Scores: (Pos: 0.521, Neg: 0.479, Neu: 0.000)\n",
            "Speaker: Sandeep, Utterance:  No, they're in fact, they will live so long. Oh shit., Sentiment: negative, Scores: (Pos: 0.002, Neg: 0.998, Neu: 0.000)\n",
            "Speaker: Ranveer, Utterance:  What do you think Adani and Mukesham money do with their own money? And by that I mean not reliance his money, Adani groups money, their own money as men., Sentiment: negative, Scores: (Pos: 0.000, Neg: 1.000, Neu: 0.000)\n",
            "Speaker: Ranveer, Utterance:  Now there's countless content pieces on YouTube on Spotify where you can learn about money, but it's these kind of conversations with experts from the world of finance people who manage money in the range of 50,000 rupees and savings to 50 crores and savings per month. That's the experience, the financial know how of our guest today. It's Sandeep Jaitwani on the show. His startup Dizerve.in is a modern day portfolio management service. Personally for me, most of my money is handled not by myself, but by portfolio management services. I trust this man enough to give him my own money. He is handling some of beer by himself media world's funds. That's why I thought that on this episode, let me give you guys access to someone who is given me financial advice, someone that I trust my money with. It's going to be a very raw and open conversation about everything from how the richest in India manages their money to how you can manage your money even if you're just beginning your career right now. It's a financial edu key., Sentiment: negative, Scores: (Pos: 0.000, Neg: 1.000, Neu: 0.000)\n",
            "\n",
            "Summary by Speaker:\n",
            "         finbert_pos  finbert_neg  finbert_neu  sentiment_score\n",
            "Speaker                                                        \n",
            "Ranveer     0.521317     2.478581     0.000101        -1.957264\n",
            "Sandeep     0.041214     1.025007     0.933779        -0.983793\n",
            "Speaker     0.000415     0.999409     0.000176        -0.998993\n",
            "\n",
            "Overall Sentiment Report:\n",
            "Overall Score: -3.94\n",
            "Overall Sentiment: negative\n"
          ]
        }
      ]
    }
  ]
}